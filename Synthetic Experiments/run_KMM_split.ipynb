{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01dfd0d8-e02a-49d5-9353-2574ba51c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modelsli1 import *\n",
    "import json\n",
    "import numpy as np\n",
    "from ate import*\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af5d1485-9ebd-42f5-83f0-a1aab5078491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(ker, X1, X2, gamma):\n",
    "    \"\"\"\n",
    "    Kernel function to compute kernel matrix based on kernel type.\n",
    "    :param ker: 'linear' | 'rbf'\n",
    "    :param X1: First dataset (Xs or Xt)\n",
    "    :param X2: Second dataset (Xs or Xt)\n",
    "    :param gamma: Kernel bandwidth (only used for 'rbf')\n",
    "    :return: Computed kernel matrix\n",
    "    \"\"\"\n",
    "    K = None\n",
    "    if ker == 'linear':\n",
    "        if X2 is not None:\n",
    "            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1), np.asarray(X2))\n",
    "        else:\n",
    "            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1))\n",
    "    elif ker == 'rbf':\n",
    "        if X2 is not None:\n",
    "            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1), np.asarray(X2), gamma)\n",
    "        else:\n",
    "            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1), None, gamma)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc53423-9ac1-459c-b6b8-a6592c2f58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMM:\n",
    "    def __init__(self, kernel_type='linear', gamma=1.0, B=1.0, eps=None):\n",
    "        '''\n",
    "        Initialization function\n",
    "        :param kernel_type: 'linear' | 'rbf'\n",
    "        :param gamma: kernel bandwidth for rbf kernel\n",
    "        :param B: bound for beta\n",
    "        :param eps: bound for sigma_beta\n",
    "        '''\n",
    "        self.kernel_type = kernel_type\n",
    "        self.gamma = gamma\n",
    "        self.B = B\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, Xs, Xt):\n",
    "        '''\n",
    "        Fit source and target using KMM (compute the coefficients)\n",
    "        :param Xs: ns * dim\n",
    "        :param Xt: nt * dim\n",
    "        :return: Coefficients (Pt / Ps) value vector (Beta in the paper)\n",
    "        '''\n",
    "        ns = Xs.shape[0]\n",
    "        nt = Xt.shape[0]\n",
    "        if self.eps is None:\n",
    "            self.eps = self.B / np.sqrt(ns)\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = kernel(self.kernel_type, Xs, None, self.gamma)\n",
    "        kappa = np.sum(kernel(self.kernel_type, Xs, Xt, self.gamma) * float(ns) / float(nt), axis=1)\n",
    "        \n",
    "        # Set up and solve the quadratic programming problem\n",
    "        K = matrix(K.astype(np.double))\n",
    "        kappa = matrix(kappa.astype(np.double))\n",
    "        G = matrix(np.r_[np.ones((1, ns)), -np.ones((1, ns)), np.eye(ns), -np.eye(ns)])\n",
    "        h = matrix(np.r_[ns * (1 + self.eps), ns * (self.eps - 1), self.B * np.ones((ns,)), np.zeros((ns,))])\n",
    "\n",
    "        sol = solvers.qp(K, -kappa, G, h)\n",
    "        beta = np.array(sol['x'])\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00dd30a6-b143-4f66-99cd-99a081a39c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmm(Xs, Ys, Xt, Yt, kernel_type='rbf', gamma=1.0, B=1.0):\n",
    "    \"\"\"\n",
    "    Apply KMM to source and target domain data to compute new source data.\n",
    "    :param Xs: Source data (ns * dim)\n",
    "    :param Ys: Source labels (ns * 1)\n",
    "    :param Xt: Target data (nt * dim)\n",
    "    :param Yt: Target labels (nt * 1)\n",
    "    :param kernel_type: 'linear' | 'rbf', default is 'rbf'\n",
    "    :param gamma: Bandwidth parameter for 'rbf' kernel, default is 1.0\n",
    "    :param B: Bound for beta, default is 1.0\n",
    "    :return: New source data Xs_new after applying KMM\n",
    "    \"\"\"\n",
    "    # Initialize KMM model\n",
    "    kmm = KMM(kernel_type=kernel_type, gamma=gamma, B=B)\n",
    "    \n",
    "    # Fit KMM model to compute the coefficients\n",
    "    beta = kmm.fit(Xs, Xt)\n",
    "    \n",
    "    # Compute the new source data Xs_new by scaling the original Xs with beta\n",
    "    Xs_new = beta * Xs\n",
    "    \n",
    "    return Xs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7312129-4e81-4f9a-821b-67b726821a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入数据\n",
    "def load_targ(file_path='/Users/asus/Desktop/SCM/targ.csv'):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        if content.startswith('\\ufeff'):  # 检查并去除 BOM\n",
    "            content = content[1:]\n",
    "\n",
    "    # 使用 np.loadtxt 读取文件内容（处理了 BOM 后的内容）\n",
    "    from io import StringIO\n",
    "    data = np.loadtxt(StringIO(content), delimiter=',')\n",
    "    x=data[:, 2:]\n",
    "    t, y,  = data[:, 0], data[:, 1][:, None]\n",
    "    return x,t.reshape(-1, 1),y\n",
    "def load_sour1(file_path='/Users/asus/Desktop/SCM/sour1.csv'):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        if content.startswith('\\ufeff'):  # 检查并去除 BOM\n",
    "            content = content[1:]\n",
    "\n",
    "    # 使用 np.loadtxt 读取文件内容（处理了 BOM 后的内容）\n",
    "    from io import StringIO\n",
    "    data1 = np.loadtxt(StringIO(content), delimiter=',')\n",
    "    x=data1[:, 2:]\n",
    "    t, y,  = data1[:, 0], data1[:, 1][:, None]\n",
    "    return x,t.reshape(-1, 1),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4112f58-5c26-4693-b945-2a45482018db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimate(q_t0, q_t1, g, t, y_dragon, index, eps, truncate_level=0.01):\n",
    "    \"\"\"\n",
    "    getting the back door adjustment & TMLE estimation\n",
    "    \"\"\"\n",
    "\n",
    "    psi_n = psi_naive(q_t0, q_t1, g, t, y_dragon, truncate_level=truncate_level)\n",
    "    ipw_n, dr_n = psi_weighting(q_t0, q_t1, g, t, y_dragon, truncate_level=truncate_level)\n",
    "    psi_tmle, psi_tmle_std, eps_hat, initial_loss, final_loss, g_loss = psi_tmle_cont_outcome(q_t0, q_t1, g, t,\n",
    "                                                                                              y_dragon,\n",
    "                                                                                              truncate_level=truncate_level)\n",
    "    return psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebfdec6-9b9d-48d1-b4d9-ddeb2c1b2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd2259d-89ab-4cf3-b7c6-419810fbf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_output(yt_hat, t, y, y_scaler, x, index):\n",
    "    \"\"\"\n",
    "        Split output into dictionary for easier use in estimation#为了以后方便使用\n",
    "        Args:\n",
    "            yt_hat: Generated prediction，生成的预测，有两个y0与y1\n",
    "            t: Binary treatment assignments\n",
    "            y: Treatment outcomes,实际已有的数据\n",
    "            y_scaler: Scaled treatment outcomes#标准化后的\n",
    "            x: Covariates\n",
    "            index: Index in data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of all needed data\n",
    "    \"\"\"\n",
    "    yt_hat = yt_hat.detach().cpu().numpy()#将 yt_hat 从 PyTorch 的张量转换成 NumPy 数组（脱离计算图，移到 CPU）。\n",
    "    q_t0 = y_scaler.inverse_transform(yt_hat[:, 0].reshape(-1, 1).copy())#归一化后的对照组潜在预测结果\n",
    "    q_t1 = y_scaler.inverse_transform(yt_hat[:, 1].reshape(-1, 1).copy())\n",
    "    g = yt_hat[:, 2].copy()\n",
    "\n",
    "    if yt_hat.shape[1] == 4:\n",
    "        eps = yt_hat[:, 3][0]# 如果 `yt_hat` 有第四列，提取 `eps`\n",
    "    else:\n",
    "        eps = np.zeros_like(yt_hat[:, 2])\n",
    "\n",
    "    y = y_scaler.inverse_transform(y.copy())\n",
    "    var = \"average propensity for treated: {} and untreated: {}\".format(g[t.squeeze() == 1.].mean(),\n",
    "                                                                        g[t.squeeze() == 0.].mean())\n",
    "    print(var)\n",
    "\n",
    "    return {'q_t0': q_t0, 'q_t1': q_t1, 'g': g, 't': t, 'y': y, 'x': x, 'index': index, 'eps': eps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1918ef8a-72f2-4ead-bda4-864db27f5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, optimizer, criterion,valid_loader= None,l1_reg = None):\n",
    "    \"\"\"\n",
    "    Trains network for one epoch in batches.\n",
    "    Args:\n",
    "        train_loader: Data loader for training set.\n",
    "        net: Neural network model.\n",
    "        optimizer: Optimizer (e.g. SGD).优化器\n",
    "        criterion: Loss function (e.g. cross-entropy loss).\n",
    "    \"\"\"\n",
    "\n",
    "    avg_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]，获取输入;data 是 [inputs， labels] 的列表\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients，参数梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize，前进、反馈、最优化\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if l1_reg is not None:\n",
    "            l1_penalty = l1_reg * sum([p.abs().sum() for p in net.parameters()])\n",
    "            loss+= l1_penalty\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of loss and accuracy，跟踪损失和准确性\n",
    "        avg_loss += loss\n",
    "\n",
    "    valid_loss = None\n",
    "    if valid_loader is not None:\n",
    "        valid_loss = 0.0\n",
    "        net.eval()     # Optional when not using Model Specific layer，不使用模型特定图层时可选\n",
    "        for data, labels in valid_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "            \n",
    "            target = net(data)#加进了目标域？\n",
    "            loss = criterion(target,labels)\n",
    "            if l1_reg is not None:\n",
    "                loss+= l1_reg * sum([p.abs().sum() for p in net.parameters()])\n",
    "\n",
    "            valid_loss += loss\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "    return avg_loss / len(train_loader), valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61823478-d79c-4a13-8d70-043975650dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_dragons(t, y_unscaled, x, net,seed = 0, targeted_regularization=True, output_dir='',\n",
    "                              knob_loss=dragonnet_loss_binarycross, ratio=1., dragon='', val_split=0.2, batch_size=64,lr =1e-3,l1_reg = None):\n",
    "    \"\"\"\n",
    "    Method for training dragonnet and tarnet and predicting new results\n",
    "    Returns:\n",
    "        Outputs on train and test data\n",
    "用于训练 dragonnet 和 tarnet 并预测新结果的方法，\n",
    "返回： train 和 test 数据的输出\n",
    "    \"\"\"    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    verbose = 0\n",
    "    y_scaler = StandardScaler()\n",
    "    y = y_scaler.fit_transform(y_unscaled)\n",
    "    train_outputs = []#定义元组，将输出存在里面\n",
    "    test_outputs = []\n",
    "\n",
    "\n",
    "    # Which loss to use for training the network，选择损失函数，在dragonnet_loss与普通knob_loss中选取\n",
    "    if targeted_regularization:\n",
    "        loss = make_tarreg_loss(ratio=ratio, dragonnet_loss=knob_loss)\n",
    "    else:\n",
    "        loss = knob_loss\n",
    "\n",
    "\n",
    "    i = seed\n",
    "    torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "    if ratio == 0:\n",
    "        train_index = np.arange(x.shape[0])\n",
    "        test_index = train_index\n",
    "    else:\n",
    "        train_index, test_index = train_test_split(np.arange(x.shape[0]), test_size=ratio, random_state=seed)\n",
    "        print(f'test_index {test_index}')\n",
    "   \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    t_train, t_test = t[train_index], t[test_index]\n",
    "\n",
    "    yt_train = np.concatenate([y_train, t_train], 1)\n",
    "\n",
    "    yt_test = np.concatenate([y_test, t_test], 1)\n",
    "\n",
    "    # Create data loader to pass onto training method，创建数据加载器以传递到训练方法\n",
    "    tensors_train = torch.from_numpy(x_train).float().to(device), torch.from_numpy(yt_train).float().to(device)\n",
    "    train_size = int((val_split) * len(TensorDataset(*tensors_train)))\n",
    "    val_size = int(len(TensorDataset(*tensors_train))-train_size)\n",
    "    train_set, valid_set = random_split(TensorDataset(*tensors_train),[train_size,val_size])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=500)\n",
    "\n",
    "    import time;\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configuring optimizers，配置优化器，惩罚迭代过程\n",
    "    # Training the networks first for 100 epochs with the Adam optimizer and，首先使用 Adam 优化器训练网络 100 个 epoch\n",
    "    # then for 300 epochs with the SGD optimizer.Adam 用于初始阶段，SGD 用于更长的训练阶段\n",
    "    epochs1 = 100\n",
    "    epochs2 = 300\n",
    "\n",
    "    # Add L2 regularization to t0 and t1 heads of the network，将 L2 正则化添加到网络的 t0 和 t1 头\n",
    "    optimizer_Adam = optim.Adam([{'params': net.representation_block.parameters()},\n",
    "                                 {'params': net.t_predictions.parameters()},\n",
    "                                 {'params': net.t0_head.parameters(), 'weight_decay': 0.01},\n",
    "                                 {'params': net.t1_head.parameters(), 'weight_decay': 0.01}], lr=lr)\n",
    "    optimizer_SGD = optim.SGD([{'params': net.representation_block.parameters()},\n",
    "                               {'params': net.t_predictions.parameters()},\n",
    "                               {'params': net.t0_head.parameters(), 'weight_decay': 0.01},\n",
    "                               {'params': net.t1_head.parameters(), 'weight_decay': 0.01}], lr=lr*0.01, momentum=0.9)\n",
    "    scheduler_Adam = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_Adam, mode='min', factor=0.5, patience=5,\n",
    "                                                          threshold=1e-8, cooldown=0, min_lr=0)\n",
    "    scheduler_SGD = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_SGD, mode='min', factor=0.5, patience=5,\n",
    "                                                         threshold=0, cooldown=0, min_lr=0)\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    early_stopper = EarlyStopper(patience=2, min_delta=0.)\n",
    "\n",
    "    # Adam training run\n",
    "    for epoch in range(epochs1):\n",
    "\n",
    "        # Train on data\n",
    "        train_loss,val_loss = train(train_loader, net, optimizer_Adam, loss,valid_loader = valid_loader,l1_reg = l1_reg)\n",
    "        \n",
    "        if early_stopper.early_stop(val_loss):             \n",
    "            break\n",
    "\n",
    "        scheduler_Adam.step(val_loss)\n",
    "\n",
    "    print(f\"Adam loss: train -- {train_loss}, validation -- {val_loss}, epoch {epoch}\")\n",
    "\n",
    "    # SGD training run\n",
    "    \n",
    "    early_stopper = EarlyStopper(patience=40, min_delta=0.)\n",
    "\n",
    "    for epoch in range(epochs2):\n",
    "        # Train on data\n",
    "        train_loss,val_loss = train(train_loader, net, optimizer_SGD, loss,valid_loader = valid_loader,l1_reg = l1_reg)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):             \n",
    "            break\n",
    "        scheduler_SGD.step(val_loss)\n",
    "        \n",
    "\n",
    "    print(f\"SGD loss: train --  {train_loss}, validation -- {val_loss},  epoch {epoch}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"***************************** elapsed_time is: \", elapsed_time)\n",
    "#对训练集和测试集生成预测，并使用 _split_output 进行拆分，存储在 train_outputs 和 test_outputs 中。\n",
    "    yt_hat_test = net(torch.from_numpy(x_test).float().to(device))\n",
    "    yt_hat_train = net(torch.from_numpy(x_train).float().to(device))\n",
    "\n",
    "    test_outputs += [_split_output(yt_hat_test, t_test, y_test, y_scaler, x_test, test_index)]\n",
    "    train_outputs += [_split_output(yt_hat_train, t_train, y_train, y_scaler, x_train, train_index)]\n",
    "   \n",
    "    train_all_dicts = _split_output(yt_hat_train, t_train, y_train, y_scaler, x_train, train_index)\n",
    "    test_all_dicts = _split_output(yt_hat_test, t_test, y_test, y_scaler, x_test, test_index)\n",
    "#使用 get_estimate 计算因果推断指标（如 psi_n、tmle、ipw_n 等），并将这些结果存储在 train_dict 和 test_dict 中。    \n",
    "    psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n = get_estimate(train_all_dicts['q_t0'].reshape(-1, 1), train_all_dicts['q_t1'].reshape(-1, 1), train_all_dicts['g'].reshape(-1, 1), train_all_dicts['t'].reshape(-1, 1), train_all_dicts['y'].reshape(-1, 1), train_all_dicts['index'].reshape(-1, 1), train_all_dicts['eps'].reshape(-1, 1),truncate_level=0.01)\n",
    "\n",
    "    train_dict = {'psi_n':psi_n, 'classification_mse': g_loss,'ipw_n':ipw_n, 'dr_n':dr_n,'regression_loss':regression_loss(torch.tensor(yt_train).to(device),yt_hat_train).cpu().detach(),'BCE':binary_classification_loss(torch.tensor(yt_train).float().to(device),yt_hat_train).cpu().detach().numpy(),'regression_mse':initial_loss,'index':train_all_dicts['index']}\n",
    "    \n",
    "    psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n = get_estimate(test_all_dicts['q_t0'].reshape(-1, 1), test_all_dicts['q_t1'].reshape(-1, 1), test_all_dicts['g'].reshape(-1, 1), test_all_dicts['t'].reshape(-1, 1), test_all_dicts['y'].reshape(-1, 1), test_all_dicts['index'].reshape(-1, 1), test_all_dicts['eps'].reshape(-1, 1),truncate_level=0.01)\n",
    "\n",
    "    \n",
    "    test_dict = {'psi_n':psi_n, 'classification_mse': g_loss,'ipw_n':ipw_n, 'dr_n':dr_n,'regression_loss':regression_loss(torch.tensor(yt_test).to(device),yt_hat_test).cpu().detach(),'BCE':binary_classification_loss(torch.tensor(yt_test).float().to(device),yt_hat_test).cpu().detach().numpy(),'regression_mses':initial_loss,'index':test_all_dicts['index']}\n",
    "#trans相比 多了 多的是与插件的结合，输出多了  train_dict,test_dict\n",
    "    return test_outputs, train_outputs, net,train_dict,test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4c76f3-2412-49a1-afc1-7797a7b24425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"递归地将不可序列化的对象转换为 JSON 可序列化的形式\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(i) for i in obj]\n",
    "    elif isinstance(obj, torch.Tensor):  # 处理 PyTorch Tensor\n",
    "        return obj.tolist() if obj.dim() > 0 else obj.item()\n",
    "    elif isinstance(obj, np.ndarray):  # NumPy 数组转列表\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):  # NumPy 浮点转标准浮点\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):  # NumPy 整数转标准整数\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2748be3e-45bd-4c73-9736-f94cdee7e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_KMMsplit(data_base_dir='/Users/asus/Desktop/SCM', output_dir='/Users/asus/Desktop/SCM',\n",
    "             knob_loss=dragonnet_loss_binarycross, ratio=1., dragon='', lr2=1e-3, l1_reg=1e-3, batchsize2=64):\n",
    "    print(\"the dragon is {}\".format(dragon))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x_t, t_t, y_t = load_targ()\n",
    "    x_s, t_s, y_s = load_sour1()\n",
    "    # 按照二分类数据 t 列进行分类\n",
    "    # 确保 t 是一维数组并且只用于行索引\n",
    "    x_t0, y_t0 = x_t[t_t.ravel() == 0], y_t[t_t.ravel() == 0]#目标域的处理与对照的划分\n",
    "    x_t1, y_t1 = x_t[t_t.ravel() == 1], y_t[t_t.ravel() == 1]\n",
    "    x_s0, y_s0 = x_s[t_s.ravel() == 0], y_s[t_s.ravel() == 0]#源域的处理与对照的划分\n",
    "    x_s1, y_s1 = x_s[t_s.ravel() == 1], y_s[t_s.ravel() == 1]\n",
    "    x_s0_new = apply_kmm(x_t0, y_t0, x_s0, y_s0, kernel_type='rbf', gamma=1, B=8)\n",
    "    x_s1_new = apply_kmm(x_t1, y_t1, x_s1, y_s1, kernel_type='rbf', gamma=1, B=8)\n",
    "\n",
    "        # 合并新源域数据\n",
    "    Xs_new = np.vstack((x_s0_new, x_s1_new))\n",
    " \n",
    "    final_output = []\n",
    "    for is_targeted_regularization in [False]:\n",
    "        print(\"Is targeted regularization: {}\".format(is_targeted_regularization))\n",
    "        net = TarNet(x_s.shape[1]).to(device) if dragon == 'tarnet' else DragonNet(x_s.shape[1]).to(device)\n",
    "        \n",
    "        # Train source data\n",
    "        _, _, net, _, _ = train_and_predict_dragons(t_s, y_s, Xs_new, net, seed=42,\n",
    "                                                    targeted_regularization=is_targeted_regularization,\n",
    "                                                    knob_loss=knob_loss, ratio=0, dragon=dragon,\n",
    "                                                    val_split=0.3, batch_size=64, lr=1e-3)\n",
    "        \n",
    "        parm = {}\n",
    "        for name, param in net.named_parameters():\n",
    "            param.grad = None\n",
    "            parm[name]=param.detach()\n",
    "\n",
    "        if dragon == 'tarnet':\n",
    "            print('I am here making tarnet')\n",
    "            net = TarNet_transfer(x_s.shape[1],parm).to(device)\n",
    "\n",
    "        elif dragon == 'dragonnet':\n",
    "            print(\"I am here making dragonnet\")\n",
    "            net = DragonNet_transfer(x_s.shape[1],parm).to(device)\n",
    "        \n",
    "        # Train target data\n",
    "        test_outputs, train_output, net, train_dict, test_dict = train_and_predict_dragons(\n",
    "            t_t, y_t, x_t, net, seed=42, targeted_regularization=is_targeted_regularization,\n",
    "            knob_loss=knob_loss, ratio=0.5, dragon=dragon, val_split=0.3, batch_size=batchsize2, lr=lr2, l1_reg=l1_reg)\n",
    "        \n",
    "        # Calculate errors\n",
    "        for result_dict in [train_dict, test_dict]:\n",
    "            truth = 2\n",
    "            result_dict['index'] = result_dict['index'].tolist()\n",
    "            result_dict['err'] = abs(truth - result_dict['psi_n']).mean()\n",
    "            result_dict['dr_err'] = abs(truth - result_dict['dr_n']).mean()\n",
    "            result_dict['ipw_error'] = abs(truth - result_dict['ipw_n']).mean()\n",
    "        train_dict = {f'{k}_train': v.item() if 'index' not in k else v for k, v in train_dict.items()}\n",
    "        test_dict = {f'{k}_test': v.item() if 'index' not in k else v for k, v in test_dict.items()}\n",
    "        train_dict = {**train_dict,**test_dict}\n",
    "        \n",
    "\n",
    "        final_output.append(train_dict)\n",
    "    \n",
    "    # Save results\n",
    "    serializable_output = convert_to_serializable(final_output)\n",
    "    if not os.path.exists(f'./KMM-split-mild-params_target{1}/'):\n",
    "        os.makedirs(f'./KMM-split-mild-params_target{1}/')\n",
    "    with open(f'./KMM-split-mild-params_target{1}/KMM-split-severe-experiments_transfer.json', 'w') as fp:\n",
    "        json.dump(serializable_output, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "903986d1-6788-458b-afec-13a5f8cf50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_knob(data_base_dir='/Users/asus/Desktop/SCM/', knob='dragonnet',\n",
    "              output_base_dir='',lr  = 1e-3, l1reg = 1e-4,batchsize = 64):\n",
    "    output_dir = os.path.join(output_base_dir, knob)\n",
    "\n",
    "    if knob == 'dragonnet':\n",
    "        run_KMMsplit(data_base_dir=data_base_dir, output_dir=output_dir, dragon='dragonnet' ,lr2  = lr ,l1_reg = l1reg, batchsize2 = batchsize)\n",
    "\n",
    "    if knob == 'tarnet':\n",
    "        run_KMMsplit(data_base_dir=data_base_dir, output_dir=output_dir, dragon='tarnet',lr2  = lr ,l1_reg = l1reg, batchsize2 = batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ab6b26-8517-4bc3-b2d4-c02a001d962a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dragon is dragonnet\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.2002e+02 -5.6789e+04  7e+04  8e-02  1e-15\n",
      " 1: -9.2289e+01 -5.5402e+03  5e+03  2e-16  1e-15\n",
      " 2: -6.2620e+02 -1.3644e+03  7e+02  2e-16  6e-16\n",
      " 3: -7.5333e+02 -8.6454e+02  1e+02  1e-16  3e-16\n",
      " 4: -7.7356e+02 -7.8639e+02  1e+01  1e-16  3e-16\n",
      " 5: -7.7702e+02 -7.8120e+02  4e+00  1e-16  3e-16\n",
      " 6: -7.7768e+02 -7.7901e+02  1e+00  1e-16  3e-16\n",
      " 7: -7.7797e+02 -7.7843e+02  5e-01  2e-16  3e-16\n",
      " 8: -7.7807e+02 -7.7820e+02  1e-01  8e-17  3e-16\n",
      " 9: -7.7811e+02 -7.7813e+02  3e-02  3e-16  4e-16\n",
      "10: -7.7812e+02 -7.7812e+02  1e-03  7e-17  3e-16\n",
      "11: -7.7812e+02 -7.7812e+02  6e-05  2e-16  3e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.3345e+02 -6.4401e+04  1e+05  2e-01  4e-15\n",
      " 1:  5.5208e+02 -1.6589e+04  2e+04  1e-16  1e-15\n",
      " 2: -5.0820e+02 -8.8306e+03  8e+03  2e-16  9e-16\n",
      " 3: -8.2871e+02 -2.8618e+03  2e+03  2e-16  7e-16\n",
      " 4: -9.7376e+02 -1.4296e+03  5e+02  1e-16  3e-16\n",
      " 5: -1.0226e+03 -1.0850e+03  6e+01  2e-16  3e-16\n",
      " 6: -1.0312e+03 -1.0402e+03  9e+00  3e-16  3e-16\n",
      " 7: -1.0329e+03 -1.0370e+03  4e+00  2e-16  3e-16\n",
      " 8: -1.0336e+03 -1.0343e+03  7e-01  1e-16  3e-16\n",
      " 9: -1.0337e+03 -1.0340e+03  2e-01  7e-17  3e-16\n",
      "10: -1.0338e+03 -1.0339e+03  9e-02  1e-16  4e-16\n",
      "11: -1.0338e+03 -1.0338e+03  2e-02  2e-16  4e-16\n",
      "12: -1.0338e+03 -1.0338e+03  8e-03  2e-16  4e-16\n",
      "13: -1.0338e+03 -1.0338e+03  1e-03  2e-16  4e-16\n",
      "14: -1.0338e+03 -1.0338e+03  1e-04  2e-16  3e-16\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "cuda:0\n",
      "Adam loss: train -- 76.06156921386719, validation -- 231.86972045898438, epoch 4\n",
      "SGD loss: train --  74.60426330566406, validation -- 226.010498046875,  epoch 133\n",
      "***************************** elapsed_time is:  1.1379187107086182\n",
      "average propensity for treated: 0.7337563633918762 and untreated: 0.7121147513389587\n",
      "average propensity for treated: 0.7337563633918762 and untreated: 0.7121147513389587\n",
      "average propensity for treated: 0.7337563633918762 and untreated: 0.7121147513389587\n",
      "average propensity for treated: 0.7337563633918762 and untreated: 0.7121147513389587\n",
      "I am here making dragonnet\n",
      "cuda:0\n",
      "test_index [ 95  15  30 158 128 115  69 170 174  45  66 182 165  78 186 177  56 152\n",
      "  82  68 124  16 148  93  65  60  84  67 125 132   9  18  55  75 150 104\n",
      " 135 137 164  76  79 197  38  24 122 195  29  19 143  86 114 173   5 126\n",
      " 117  73 140  98 172  96 169  97  31  12  35 119  42 189  90 136  51 127\n",
      " 162  41 118 113  26 139 100 111   2  77  46 187 191  85 161  36 190  61\n",
      "  22 141 101  33  11 194 159   6  27 120]\n",
      "Adam loss: train -- 23.591161727905273, validation -- 78.30924987792969, epoch 32\n",
      "SGD loss: train --  9.187007904052734, validation -- 63.82335662841797,  epoch 299\n",
      "***************************** elapsed_time is:  12.272475719451904\n",
      "average propensity for treated: 0.5885040163993835 and untreated: 0.4512985646724701\n",
      "average propensity for treated: 0.5722552537918091 and untreated: 0.42667102813720703\n",
      "average propensity for treated: 0.5722552537918091 and untreated: 0.42667102813720703\n",
      "average propensity for treated: 0.5885040163993835 and untreated: 0.4512985646724701\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_base_dir', type=str, help=\"path to directory LBIDD\", default=\"/Users/asus/Desktop/SCM\")\n",
    "    parser.add_argument('--knob', type=str, default='dragonnet',\n",
    "                        help=\"dragonnet or tarnet\")\n",
    "\n",
    "    parser.add_argument('--output_base_dir', type=str, help=\"directory to save the output\",default=\"/Users/asus/Desktop/SCM\")\n",
    "\n",
    "    parser.add_argument('--transfer_lr',type = float,default=0.01)\n",
    "\n",
    "    parser.add_argument('--l1reg',type = float,default=0.1)\n",
    "\n",
    "    parser.add_argument('--batchsize',type = int,default=64)\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    turn_knob(args.data_base_dir, args.knob, args.output_base_dir,args.transfer_lr, args.l1reg,args.batchsize)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605c7cc-346a-4eec-b63a-dea4f2be8f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f58376a-4cac-4038-9875-ccfe576e751f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
