{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96590fbe-e877-4fbd-ad7f-0897c5dc23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modelsli1 import *\n",
    "from ihdp_data import *\n",
    "import json\n",
    "import numpy as np\n",
    "from ate import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efece368-2459-40fb-9004-f537d2751c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(ker, X1, X2, gamma):\n",
    "    \"\"\"\n",
    "    Kernel function to compute kernel matrix based on kernel type.\n",
    "    :param ker: 'linear' | 'rbf'\n",
    "    :param X1: First dataset (Xs or Xt)\n",
    "    :param X2: Second dataset (Xs or Xt)\n",
    "    :param gamma: Kernel bandwidth (only used for 'rbf')\n",
    "    :return: Computed kernel matrix\n",
    "    \"\"\"\n",
    "    K = None\n",
    "    if ker == 'linear':\n",
    "        if X2 is not None:\n",
    "            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1), np.asarray(X2))\n",
    "        else:\n",
    "            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1))\n",
    "    elif ker == 'rbf':\n",
    "        if X2 is not None:\n",
    "            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1), np.asarray(X2), gamma)\n",
    "        else:\n",
    "            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1), None, gamma)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e0c725-c301-426e-b548-29f2a1e63fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMM:\n",
    "    def __init__(self, kernel_type='linear', gamma=1.0, B=1.0, eps=None):\n",
    "        '''\n",
    "        Initialization function\n",
    "        :param kernel_type: 'linear' | 'rbf'\n",
    "        :param gamma: kernel bandwidth for rbf kernel\n",
    "        :param B: bound for beta\n",
    "        :param eps: bound for sigma_beta\n",
    "        '''\n",
    "        self.kernel_type = kernel_type\n",
    "        self.gamma = gamma\n",
    "        self.B = B\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, Xs, Xt):\n",
    "        '''\n",
    "        Fit source and target using KMM (compute the coefficients)\n",
    "        :param Xs: ns * dim\n",
    "        :param Xt: nt * dim\n",
    "        :return: Coefficients (Pt / Ps) value vector (Beta in the paper)\n",
    "        '''\n",
    "        ns = Xs.shape[0]\n",
    "        nt = Xt.shape[0]\n",
    "        if self.eps is None:\n",
    "            self.eps = self.B / np.sqrt(ns)\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = kernel(self.kernel_type, Xs, None, self.gamma)\n",
    "        kappa = np.sum(kernel(self.kernel_type, Xs, Xt, self.gamma) * float(ns) / float(nt), axis=1)\n",
    "        \n",
    "        # Set up and solve the quadratic programming problem\n",
    "        K = matrix(K.astype(np.double))\n",
    "        kappa = matrix(kappa.astype(np.double))\n",
    "        G = matrix(np.r_[np.ones((1, ns)), -np.ones((1, ns)), np.eye(ns), -np.eye(ns)])\n",
    "        h = matrix(np.r_[ns * (1 + self.eps), ns * (self.eps - 1), self.B * np.ones((ns,)), np.zeros((ns,))])\n",
    "\n",
    "        sol = solvers.qp(K, -kappa, G, h)\n",
    "        beta = np.array(sol['x'])\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4748677-3a53-4768-8aa2-f298a11ad910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmm(Xs, Ys, Xt, Yt, kernel_type='rbf', gamma=1.0, B=1.0):\n",
    "    \"\"\"\n",
    "    Apply KMM to source and target domain data to compute new source data.\n",
    "    :param Xs: Source data (ns * dim)\n",
    "    :param Ys: Source labels (ns * 1)\n",
    "    :param Xt: Target data (nt * dim)\n",
    "    :param Yt: Target labels (nt * 1)\n",
    "    :param kernel_type: 'linear' | 'rbf', default is 'rbf'\n",
    "    :param gamma: Bandwidth parameter for 'rbf' kernel, default is 1.0\n",
    "    :param B: Bound for beta, default is 1.0\n",
    "    :return: New source data Xs_new after applying KMM\n",
    "    \"\"\"\n",
    "    # Initialize KMM model\n",
    "    kmm = KMM(kernel_type=kernel_type, gamma=gamma, B=B)\n",
    "    \n",
    "    # Fit KMM model to compute the coefficients\n",
    "    beta = kmm.fit(Xs, Xt)\n",
    "    \n",
    "    # Compute the new source data Xs_new by scaling the original Xs with beta\n",
    "    Xs_new = beta * Xs\n",
    "    \n",
    "    return Xs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49ab099-5d82-47b8-bf20-6588dca3ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimate(q_t0, q_t1, g, t, y_dragon, index, eps, truncate_level=0.01):\n",
    "    \"\"\"\n",
    "    getting the back door adjustment & TMLE estimation\n",
    "    \"\"\"\n",
    "\n",
    "    psi_n = psi_naive(q_t0, q_t1, g, t, y_dragon, truncate_level=truncate_level)\n",
    "    ipw_n, dr_n = psi_weighting(q_t0, q_t1, g, t, y_dragon, truncate_level=truncate_level)\n",
    "    psi_tmle, psi_tmle_std, eps_hat, initial_loss, final_loss, g_loss = psi_tmle_cont_outcome(q_t0, q_t1, g, t,\n",
    "                                                                                              y_dragon,\n",
    "                                                                                              truncate_level=truncate_level)\n",
    "    return psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a367ee-8dd4-4ac3-a824-82468e181c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735ae630-de5c-4780-aba6-27bc488422f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_output(yt_hat, t, y, y_scaler, x, index):\n",
    "    \"\"\"\n",
    "        Split output into dictionary for easier use in estimation#为了以后方便使用\n",
    "        Args:\n",
    "            yt_hat: Generated prediction，生成的预测，有两个y0与y1\n",
    "            t: Binary treatment assignments\n",
    "            y: Treatment outcomes,实际已有的数据\n",
    "            y_scaler: Scaled treatment outcomes#标准化后的\n",
    "            x: Covariates\n",
    "            index: Index in data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of all needed data\n",
    "    \"\"\"\n",
    "    yt_hat = yt_hat.detach().cpu().numpy()#将 yt_hat 从 PyTorch 的张量转换成 NumPy 数组（脱离计算图，移到 CPU）。\n",
    "    q_t0 = y_scaler.inverse_transform(yt_hat[:, 0].reshape(-1, 1).copy())#归一化后的对照组潜在预测结果\n",
    "    q_t1 = y_scaler.inverse_transform(yt_hat[:, 1].reshape(-1, 1).copy())\n",
    "    g = yt_hat[:, 2].copy()# 提取倾向得分\n",
    "\n",
    "    if yt_hat.shape[1] == 4:\n",
    "        eps = yt_hat[:, 3][0]# 如果 `yt_hat` 有第四列，提取 `eps`\n",
    "    else:\n",
    "        eps = np.zeros_like(yt_hat[:, 2])# 否则，`eps` 初始化为全零\n",
    "\n",
    "    y = y_scaler.inverse_transform(y.copy())#逆归一化\n",
    "    var = \"average propensity for treated: {} and untreated: {}\".format(g[t.squeeze() == 1.].mean(),\n",
    "                                                                        g[t.squeeze() == 0.].mean())\n",
    "    print(var)\n",
    "\n",
    "    return {'q_t0': q_t0, 'q_t1': q_t1, 'g': g, 't': t, 'y': y, 'x': x, 'index': index, 'eps': eps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf72e2e8-2e62-4544-9d1f-b188636d48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, optimizer, criterion,valid_loader= None,l1_reg = None):\n",
    "    \"\"\"\n",
    "    Trains network for one epoch in batches.\n",
    "    Args:\n",
    "        train_loader: Data loader for training set.\n",
    "        net: Neural network model.\n",
    "        optimizer: Optimizer (e.g. SGD).优化器\n",
    "        criterion: Loss function (e.g. cross-entropy loss).\n",
    "    \"\"\"\n",
    "\n",
    "    avg_loss = 0\n",
    "    #平均损失\n",
    "    # iterate through batches，迭代处理\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]，获取输入;data 是 [inputs， labels] 的列表\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients，参数梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize，前进、反馈、最优化\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if l1_reg is not None:\n",
    "            l1_penalty = l1_reg * sum([p.abs().sum() for p in net.parameters()])\n",
    "            loss+= l1_penalty\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of loss and accuracy，跟踪损失和准确性\n",
    "        avg_loss += loss\n",
    "\n",
    "    valid_loss = None\n",
    "    if valid_loader is not None:\n",
    "        valid_loss = 0.0\n",
    "        net.eval()     # Optional when not using Model Specific layer，不使用模型特定图层时可选\n",
    "        for data, labels in valid_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "            \n",
    "            target = net(data)\n",
    "            loss = criterion(target,labels)\n",
    "         \n",
    "            if l1_reg is not None:\n",
    "                loss+= l1_reg * sum([p.abs().sum() for p in net.parameters()])\n",
    "             \n",
    "            valid_loss += loss\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "    return avg_loss / len(train_loader), valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c3b416a-2f63-4ecf-8331-b1182dde6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_dragons(t, y_unscaled, x, net,seed = 0, targeted_regularization=True, output_dir='',\n",
    "                              knob_loss=dragonnet_loss_binarycross, ratio=1., dragon='', val_split=0.2, batch_size=64,lr =1e-3,l1_reg = None):\n",
    "    \"\"\"\n",
    "    Method for training dragonnet and tarnet and predicting new results\n",
    "    Returns:\n",
    "        Outputs on train and test data\n",
    "用于训练 dragonnet 和 tarnet 并预测新结果的方法，\n",
    "返回： train 和 test 数据的输出\n",
    "    \"\"\"    \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    verbose = 0\n",
    "    y_scaler = StandardScaler()\n",
    "    y = y_scaler.fit_transform(y_unscaled)\n",
    "    train_outputs = []#定义元组，将输出存在里面\n",
    "    test_outputs = []\n",
    "\n",
    "\n",
    "    # Which loss to use for training the network，选择损失函数，在dragonnet_loss与普通knob_loss中选取\n",
    "    if targeted_regularization:\n",
    "        loss = make_tarreg_loss(ratio=ratio, dragonnet_loss=knob_loss)\n",
    "    else:\n",
    "        loss = knob_loss\n",
    "\n",
    "    # loss = knob_loss\n",
    "    # for reporducing the IHDP experimemt\n",
    "\n",
    "    i = seed\n",
    "    torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "    \n",
    "\n",
    "    \n",
    " \n",
    " # Get the data and optionally divide into train and test set，获取数据并可选择划分为训练集和测试集\n",
    "\n",
    "    if ratio == 0:\n",
    "        train_index = np.arange(x.shape[0])\n",
    "        test_index = train_index\n",
    "    else:\n",
    "        train_index, test_index = train_test_split(np.arange(x.shape[0]), test_size=ratio, random_state=seed)\n",
    "        #print(f'test_index {test_index}')\n",
    "   \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    t_train, t_test = t[train_index], t[test_index]\n",
    "\n",
    "    yt_train = np.concatenate([y_train, t_train], 1)\n",
    "\n",
    "    yt_test = np.concatenate([y_test, t_test], 1)\n",
    "\n",
    "    # Create data loader to pass onto training method，创建数据加载器以传递到训练方法\n",
    "    tensors_train = torch.from_numpy(x_train).float().to(device), torch.from_numpy(yt_train).float().to(device)\n",
    "    train_size = int((val_split) * len(TensorDataset(*tensors_train)))\n",
    "    val_size = int(len(TensorDataset(*tensors_train))-train_size)\n",
    "    train_set, valid_set = random_split(TensorDataset(*tensors_train),[train_size,val_size])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=500)\n",
    "\n",
    "    import time;\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configuring optimizers，配置优化器，惩罚迭代过程\n",
    "    # Training the networks first for 100 epochs with the Adam optimizer and，首先使用 Adam 优化器训练网络 100 个 epoch\n",
    "    # then for 300 epochs with the SGD optimizer.Adam 用于初始阶段，SGD 用于更长的训练阶段\n",
    "    epochs1 = 100\n",
    "    epochs2 = 300\n",
    "#L2 regularization？\n",
    "    # Add L2 regularization to t0 and t1 heads of the network，将 L2 正则化添加到网络的 t0 和 t1 头\n",
    "    optimizer_Adam = optim.Adam([{'params': net.representation_block.parameters()},\n",
    "                                 {'params': net.t_predictions.parameters()},\n",
    "                                 {'params': net.t0_head.parameters(), 'weight_decay': 0.01},\n",
    "                                 {'params': net.t1_head.parameters(), 'weight_decay': 0.01}], lr=lr)\n",
    "    optimizer_SGD = optim.SGD([{'params': net.representation_block.parameters()},\n",
    "                               {'params': net.t_predictions.parameters()},\n",
    "                               {'params': net.t0_head.parameters(), 'weight_decay': 0.01},\n",
    "                               {'params': net.t1_head.parameters(), 'weight_decay': 0.01}], lr=lr*0.01, momentum=0.9)\n",
    "    scheduler_Adam = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_Adam, mode='min', factor=0.5, patience=5,\n",
    "                                                          threshold=1e-8, cooldown=0, min_lr=0)\n",
    "    scheduler_SGD = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_SGD, mode='min', factor=0.5, patience=5,\n",
    "                                                         threshold=0, cooldown=0, min_lr=0)\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    early_stopper = EarlyStopper(patience=2, min_delta=0.)\n",
    "\n",
    "    # Adam training run\n",
    "    for epoch in range(epochs1):\n",
    "\n",
    "        # Train on data\n",
    "        train_loss,val_loss = train(train_loader, net, optimizer_Adam, loss,valid_loader = valid_loader,l1_reg = l1_reg)\n",
    "        \n",
    "        if early_stopper.early_stop(val_loss):             \n",
    "            break\n",
    "\n",
    "        scheduler_Adam.step(val_loss)\n",
    "\n",
    "    #print(f\"Adam loss: train -- {train_loss}, validation -- {val_loss}, epoch {epoch}\")\n",
    "\n",
    "    # SGD training run\n",
    "    \n",
    "    early_stopper = EarlyStopper(patience=40, min_delta=0.)\n",
    "\n",
    "    for epoch in range(epochs2):\n",
    "        # Train on data\n",
    "        train_loss,val_loss = train(train_loader, net, optimizer_SGD, loss,valid_loader = valid_loader,l1_reg = l1_reg)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):             \n",
    "            break\n",
    "        scheduler_SGD.step(val_loss)\n",
    "        \n",
    "\n",
    "    #print(f\"SGD loss: train --  {train_loss}, validation -- {val_loss},  epoch {epoch}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print(\"***************************** elapsed_time is: \", elapsed_time)\n",
    "#对训练集和测试集生成预测，并使用 _split_output 进行拆分，存储在 train_outputs 和 test_outputs 中。\n",
    "    yt_hat_test = net(torch.from_numpy(x_test).float().to(device))\n",
    "    yt_hat_train = net(torch.from_numpy(x_train).float().to(device))\n",
    "\n",
    "    test_outputs += [_split_output(yt_hat_test, t_test, y_test, y_scaler, x_test, test_index)]\n",
    "    train_outputs += [_split_output(yt_hat_train, t_train, y_train, y_scaler, x_train, train_index)]\n",
    "   \n",
    "    train_all_dicts = _split_output(yt_hat_train, t_train, y_train, y_scaler, x_train, train_index)\n",
    "    test_all_dicts = _split_output(yt_hat_test, t_test, y_test, y_scaler, x_test, test_index)\n",
    "#使用 get_estimate 计算因果推断指标（如 psi_n、tmle、ipw_n 等），并将这些结果存储在 train_dict 和 test_dict 中。    \n",
    "    psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n = get_estimate(train_all_dicts['q_t0'].reshape(-1, 1), train_all_dicts['q_t1'].reshape(-1, 1), train_all_dicts['g'].reshape(-1, 1), train_all_dicts['t'].reshape(-1, 1), train_all_dicts['y'].reshape(-1, 1), train_all_dicts['index'].reshape(-1, 1), train_all_dicts['eps'].reshape(-1, 1),truncate_level=0.01)\n",
    "\n",
    "    train_dict = {'psi_n':psi_n, 'classification_mse': g_loss,'ipw_n':ipw_n, 'dr_n':dr_n,'regression_loss':regression_loss(torch.tensor(yt_train).to(device),yt_hat_train).cpu().detach(),'BCE':binary_classification_loss(torch.tensor(yt_train).float().to(device),yt_hat_train).cpu().detach().numpy(),'regression_mse':initial_loss,'index':train_all_dicts['index']}\n",
    "    \n",
    "    psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n = get_estimate(test_all_dicts['q_t0'].reshape(-1, 1), test_all_dicts['q_t1'].reshape(-1, 1), test_all_dicts['g'].reshape(-1, 1), test_all_dicts['t'].reshape(-1, 1), test_all_dicts['y'].reshape(-1, 1), test_all_dicts['index'].reshape(-1, 1), test_all_dicts['eps'].reshape(-1, 1),truncate_level=0.01)\n",
    "\n",
    "    \n",
    "    test_dict = {'psi_n':psi_n, 'classification_mse': g_loss,'ipw_n':ipw_n, 'dr_n':dr_n,'regression_loss':regression_loss(torch.tensor(yt_test).to(device),yt_hat_test).cpu().detach(),'BCE':binary_classification_loss(torch.tensor(yt_test).float().to(device),yt_hat_test).cpu().detach().numpy(),'regression_mses':initial_loss,'index':test_all_dicts['index']}\n",
    "\n",
    "    return test_outputs, train_outputs, net,train_dict,test_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecc03b-5507-4b9a-b047-c091cdc77385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cab2c084-8783-4045-9871-c5d37ba25161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forcing CPU-only mode\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.cuda.is_available = lambda: False\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Forcing CPU-only mode\")\n",
    "def run_KMMMerge(data_base_dir='/Users/asus/Desktop/datasets', output_dir='/Users/asus/Desktop/datasets',\n",
    "                knob_loss=dragonnet_loss_binarycross,\n",
    "                ratio=1., dragon='', lr2=1e-3, l1_reg=1e-3, batchsize2=64):\n",
    "\n",
    "    print(\"the dragon is {}\".format(dragon))\n",
    "    device = torch.device(\"cpu\")\n",
    "    simulation_files = sorted(glob.glob(\"{}/*.csv\".format(data_base_dir)))\n",
    "    \n",
    "    # 初始化列表用于收集所有测试误差\n",
    "    all_err_test = []\n",
    "    all_dr_err_test = []\n",
    "    all_ipw_error_test = []\n",
    "    \n",
    "    final_output = []\n",
    "    for idx, simulation_file in enumerate(simulation_files):\n",
    "        try:\n",
    "            print(f\"\\nProcessing file {idx+1}/{len(simulation_files)}: {os.path.basename(simulation_file)}\")\n",
    "            \n",
    "            # 加载特征和其他变量\n",
    "            x = load_and_format_covariates_ihdp(simulation_file)\n",
    "            t, y, y_cf, mu_0, mu_1 = load_all_other_crap(simulation_file)\n",
    "            \n",
    "            # 超参数设置\n",
    "            batchsize = 64\n",
    "            lr = 1e-3\n",
    "            test_ratio = 0.5\n",
    "            val_split = 0.3\n",
    "            batchsize2 = batchsize2\n",
    "            lr2 = lr2\n",
    "            l1_reg = l1_reg\n",
    "            \n",
    "            # 选择目标索引\n",
    "            target_col_idx = 3\n",
    "            target_idx0 = np.where(x[:, target_col_idx] == 0)[0]  # Source domain indices\n",
    "            target_idx1 = np.where(x[:, target_col_idx] == 1)[0]  # Target domain indices\n",
    "\n",
    "            # 划分源域和目标域数据\n",
    "            x_s = x[target_idx0]\n",
    "            y_s = y[target_idx0]\n",
    "            t_s = t[target_idx0]\n",
    "            y_cf_s = y_cf[target_idx0]\n",
    "            mu_0_s = mu_0[target_idx0]\n",
    "            mu_1_s = mu_1[target_idx0]\n",
    "\n",
    "            x_t = x[target_idx1]\n",
    "            y_t = y[target_idx1]\n",
    "            t_t = t[target_idx1]\n",
    "            y_cf_t = y_cf[target_idx1]\n",
    "            mu_0_t = mu_0[target_idx1]\n",
    "            mu_1_t = mu_1[target_idx1]\n",
    "\n",
    "            # 通过 KMM 进行域适配\n",
    "            Xs_new = apply_kmm(x_s, y_s, x_t, y_t, kernel_type='rbf', gamma=1.0, B=1.0)\n",
    "\n",
    "            # 训练源领域数据\n",
    "            for is_targeted_regularization in [False]:\n",
    "                print(\"Is targeted regularization: {}\".format(is_targeted_regularization))\n",
    "                torch.manual_seed(idx)\n",
    "\n",
    "                if dragon == 'tarnet':\n",
    "                    print('Creating TarNet model')\n",
    "                    net = TarNet(x.shape[1]).to(device)\n",
    "\n",
    "                elif dragon == 'dragonnet':\n",
    "                    print(\"Creating DragonNet model\")\n",
    "                    net = DragonNet(x.shape[1]).to(device)\n",
    "\n",
    "                # 使用更新后的 x_s（即 Xs_new）训练模型\n",
    "                _, _, net, _, _ = train_and_predict_dragons(t_s, y_s, Xs_new, net, seed=idx,\n",
    "                                                           targeted_regularization=is_targeted_regularization,\n",
    "                                                           knob_loss=knob_loss, ratio=0, dragon=dragon,\n",
    "                                                           val_split=val_split, batch_size=batchsize, lr=lr)\n",
    "                \n",
    "                parm = {}\n",
    "                for name, param in net.named_parameters():\n",
    "                    param.grad = None\n",
    "                    parm[name] = param.detach().cpu()  # 确保在CPU上\n",
    "\n",
    "                # 迁移学习阶段\n",
    "                if dragon == 'tarnet':\n",
    "                    print('Creating TarNet_transfer model')\n",
    "                    net = TarNet_transfer(x.shape[1], parm).to(device)\n",
    "\n",
    "                elif dragon == 'dragonnet':\n",
    "                    print(\"Creating DragonNet_transfer model\")\n",
    "                    net = DragonNet_transfer(x.shape[1], parm).to(device)\n",
    "\n",
    "                # 在目标域数据上进行二次训练\n",
    "                test_outputs, train_output, net, train_dict, test_dict = train_and_predict_dragons(\n",
    "                    t_t, y_t, x_t, net, seed=idx, targeted_regularization=is_targeted_regularization,\n",
    "                    knob_loss=knob_loss, ratio=test_ratio, dragon=dragon,\n",
    "                    val_split=val_split, batch_size=batchsize2, lr=lr2, l1_reg=l1_reg)\n",
    "\n",
    "                # 计算误差\n",
    "                for data_dict in [train_dict, test_dict]:\n",
    "                    # 确保索引在范围内\n",
    "                    max_index = len(mu_1_t) - 1\n",
    "                    valid_indices = [i for i in data_dict['index'] if 0 <= i <= max_index]\n",
    "                    \n",
    "                    if not valid_indices:\n",
    "                        print(f\"Warning: No valid indices in dict, skipping error calculation\")\n",
    "                        continue\n",
    "                        \n",
    "                    truth = (mu_1_t[valid_indices] - mu_0_t[valid_indices]).mean()\n",
    "                    \n",
    "                    # 确保预测值存在\n",
    "                    if 'psi_n' not in data_dict or 'dr_n' not in data_dict or 'ipw_n' not in data_dict:\n",
    "                        print(f\"Warning: Missing prediction values in dict, skipping error calculation\")\n",
    "                        continue\n",
    "                        \n",
    "                    data_dict['err'] = abs(truth - data_dict['psi_n']).mean()\n",
    "                    data_dict['dr_err'] = abs(truth - data_dict['dr_n']).mean()\n",
    "                    data_dict['ipw_error'] = abs(truth - data_dict['ipw_n']).mean()\n",
    "                    \n",
    "                    # 如果是测试集，收集误差用于最终统计\n",
    "                    if data_dict is test_dict:\n",
    "                        all_err_test.append(data_dict['err'])\n",
    "                        all_dr_err_test.append(data_dict['dr_err'])\n",
    "                        all_ipw_error_test.append(data_dict['ipw_error'])\n",
    "                \n",
    "                # 将索引转换为列表\n",
    "                test_dict['index'] = test_dict['index'].tolist()\n",
    "                train_dict['index'] = train_dict['index'].tolist()\n",
    "                \n",
    "                # 格式化输出字典\n",
    "                train_dict_formatted = {f'{k}_train': v.item() if 'index' not in k else v for k, v in train_dict.items()}\n",
    "                test_dict_formatted = {f'{k}_test': v.item() if 'index' not in k else v for k, v in test_dict.items()}\n",
    "                \n",
    "                combined_dict = {**train_dict_formatted, **test_dict_formatted}\n",
    "                combined_dict['sim_idx'] = idx\n",
    "                final_output.append(combined_dict)\n",
    "                \n",
    "                # 打印本次结果\n",
    "                print(f\"Simulation {idx} results:\")\n",
    "                print(f\"  Test err: {combined_dict.get('err_test', 'N/A'):.4f}\")\n",
    "                print(f\"  Test dr_err: {combined_dict.get('dr_err_test', 'N/A'):.4f}\")\n",
    "                print(f\"  Test ipw_error: {combined_dict.get('ipw_error_test', 'N/A'):.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {simulation_file}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # 计算所有数据集的误差统计\n",
    "    if all_err_test:\n",
    "        err_mean = np.mean(all_err_test)\n",
    "        err_var = np.var(all_err_test)\n",
    "        dr_err_mean = np.mean(all_dr_err_test)\n",
    "        dr_err_var = np.var(all_dr_err_test)\n",
    "        ipw_err_mean = np.mean(all_ipw_error_test)\n",
    "        ipw_err_var = np.var(all_ipw_error_test)\n",
    "    else:\n",
    "        # 如果没有任何成功的测试，设置默认值\n",
    "        err_mean = err_var = dr_err_mean = dr_err_var = ipw_err_mean = ipw_err_var = -1\n",
    "        print(\"WARNING: No valid test results were collected\")\n",
    "    \n",
    "    # 添加汇总统计到输出\n",
    "    summary = {\n",
    "        'err_mean': float(err_mean),\n",
    "        'err_variance': float(err_var),\n",
    "        'dr_err_mean': float(dr_err_mean),\n",
    "        'dr_err_variance': float(dr_err_var),\n",
    "        'ipw_err_mean': float(ipw_err_mean),\n",
    "        'ipw_err_variance': float(ipw_err_var),\n",
    "        'successful_runs': len(all_err_test)\n",
    "    }\n",
    "    final_output.append({'summary': summary})\n",
    "    \n",
    "    # 保存结果\n",
    "    if not os.path.exists(f'./KMMheparams_target{target_col_idx}/'):\n",
    "        os.makedirs(f'./KMMheparams_target{target_col_idx}/')\n",
    "    \n",
    "    output_file = f'./KMMheparams_target{target_col_idx}/KMMheexperiments_transfer_{dragon}_{batchsize2}_{l1_reg}_{lr2}.json'\n",
    "    \n",
    "    # 使用自定义编码器保存\n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, torch.Tensor):\n",
    "                return obj.detach().cpu().numpy().tolist()\n",
    "            return super(NumpyEncoder, self).default(obj)\n",
    "    \n",
    "    with open(output_file, 'w') as fp:\n",
    "        json.dump(final_output, fp, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    # 打印汇总结果\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"Error Summary for All Simulations:\")\n",
    "    print(f\"Successful runs: {len(all_err_test)}/{len(simulation_files)}\")\n",
    "    print(f\"ATE Error: Mean = {err_mean:.4f}, Variance = {err_var:.4f}\")\n",
    "    print(f\"DR Error: Mean = {dr_err_mean:.4f}, Variance = {dr_err_var:.4f}\")\n",
    "    print(f\"IPW Error: Mean = {ipw_err_mean:.4f}, Variance = {ipw_err_var:.4f}\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69508ae2-d89e-471f-bec8-26b4135a30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_knob(data_base_dir='/Users/asus/Desktop/datasets/', knob='dragonnet',\n",
    "              output_base_dir='',lr  = 1e-3, l1reg = 1e-4,batchsize = 16):\n",
    "    output_dir = os.path.join(output_base_dir, knob)#扩充output_dir\n",
    "\n",
    "    if knob == 'dragonnet':\n",
    "        run_KMMMerge(data_base_dir=data_base_dir, output_dir=output_dir, dragon='dragonnet' ,lr2  = lr ,l1_reg = l1reg, batchsize2 = batchsize)\n",
    "\n",
    "    if knob == 'tarnet':\n",
    "        run_KMMMerge(data_base_dir=data_base_dir, output_dir=output_dir, dragon='tarnet',lr2  = lr ,l1_reg = l1reg, batchsize2 = batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8525fdca-84bc-4668-855d-99fb625bb3aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dragon is tarnet\n",
      "\n",
      "Processing file 1/30: ihdp_shifted_1.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 43.88991165161133, validation -- 346.63372802734375, epoch 8\n",
      "SGD loss: train --  40.83441162109375, validation -- 340.44384765625,  epoch 99\n",
      "***************************** elapsed_time is:  3.080258369445801\n",
      "average propensity for treated: 0.5131701231002808 and untreated: 0.5172050595283508\n",
      "average propensity for treated: 0.5131701231002808 and untreated: 0.5172050595283508\n",
      "average propensity for treated: 0.5131701231002808 and untreated: 0.5172050595283508\n",
      "average propensity for treated: 0.5131701231002808 and untreated: 0.5172050595283508\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.535466194152832, validation -- 103.75413513183594, epoch 13\n",
      "SGD loss: train --  6.160555362701416, validation -- 103.29053497314453,  epoch 99\n",
      "***************************** elapsed_time is:  6.3301684856414795\n",
      "average propensity for treated: 0.5230432748794556 and untreated: 0.5173059105873108\n",
      "average propensity for treated: 0.5155549049377441 and untreated: 0.5253159999847412\n",
      "average propensity for treated: 0.5155549049377441 and untreated: 0.5253159999847412\n",
      "average propensity for treated: 0.5230432748794556 and untreated: 0.5173059105873108\n",
      "Simulation 0 results:\n",
      "  Test err: 1.1863\n",
      "  Test dr_err: 0.6589\n",
      "  Test ipw_error: 0.4438\n",
      "\n",
      "Processing file 2/30: ihdp_shifted_10.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 39.618953704833984, validation -- 336.6104431152344, epoch 9\n",
      "SGD loss: train --  37.46805953979492, validation -- 331.43914794921875,  epoch 88\n",
      "***************************** elapsed_time is:  2.8407785892486572\n",
      "average propensity for treated: 0.4690937399864197 and untreated: 0.46613767743110657\n",
      "average propensity for treated: 0.4690937399864197 and untreated: 0.46613767743110657\n",
      "average propensity for treated: 0.4690937399864197 and untreated: 0.46613767743110657\n",
      "average propensity for treated: 0.4690937399864197 and untreated: 0.46613767743110657\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.648379325866699, validation -- 99.03441619873047, epoch 9\n",
      "SGD loss: train --  6.48555850982666, validation -- 99.14852142333984,  epoch 40\n",
      "***************************** elapsed_time is:  2.8449532985687256\n",
      "average propensity for treated: 0.45800530910491943 and untreated: 0.4696028530597687\n",
      "average propensity for treated: 0.4615586996078491 and untreated: 0.46842527389526367\n",
      "average propensity for treated: 0.4615586996078491 and untreated: 0.46842527389526367\n",
      "average propensity for treated: 0.45800530910491943 and untreated: 0.4696028530597687\n",
      "Simulation 1 results:\n",
      "  Test err: 1.1833\n",
      "  Test dr_err: 1.3295\n",
      "  Test ipw_error: 1.0931\n",
      "\n",
      "Processing file 3/30: ihdp_shifted_11.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 44.77067565917969, validation -- 363.18463134765625, epoch 5\n",
      "SGD loss: train --  40.0300407409668, validation -- 347.62115478515625,  epoch 99\n",
      "***************************** elapsed_time is:  3.016779661178589\n",
      "average propensity for treated: 0.5065932869911194 and untreated: 0.5129791498184204\n",
      "average propensity for treated: 0.5065932869911194 and untreated: 0.5129791498184204\n",
      "average propensity for treated: 0.5065932869911194 and untreated: 0.5129791498184204\n",
      "average propensity for treated: 0.5065932869911194 and untreated: 0.5129791498184204\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.629429817199707, validation -- 95.63528442382812, epoch 5\n",
      "SGD loss: train --  6.9437575340271, validation -- 94.39645385742188,  epoch 99\n",
      "***************************** elapsed_time is:  5.843021631240845\n",
      "average propensity for treated: 0.5244942903518677 and untreated: 0.5309997797012329\n",
      "average propensity for treated: 0.5247979760169983 and untreated: 0.5188398957252502\n",
      "average propensity for treated: 0.5247979760169983 and untreated: 0.5188398957252502\n",
      "average propensity for treated: 0.5244942903518677 and untreated: 0.5309997797012329\n",
      "Simulation 2 results:\n",
      "  Test err: 1.5475\n",
      "  Test dr_err: 1.0039\n",
      "  Test ipw_error: 1.2116\n",
      "\n",
      "Processing file 4/30: ihdp_shifted_12.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 44.65461349487305, validation -- 348.87738037109375, epoch 8\n",
      "SGD loss: train --  42.097930908203125, validation -- 346.51531982421875,  epoch 70\n",
      "***************************** elapsed_time is:  2.412254571914673\n",
      "average propensity for treated: 0.5105909705162048 and untreated: 0.49848079681396484\n",
      "average propensity for treated: 0.5105909705162048 and untreated: 0.49848079681396484\n",
      "average propensity for treated: 0.5105909705162048 and untreated: 0.49848079681396484\n",
      "average propensity for treated: 0.5105909705162048 and untreated: 0.49848079681396484\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.936378479003906, validation -- 96.85029602050781, epoch 2\n",
      "SGD loss: train --  6.951844692230225, validation -- 94.58160400390625,  epoch 99\n",
      "***************************** elapsed_time is:  5.5407421588897705\n",
      "average propensity for treated: 0.5203579664230347 and untreated: 0.530987024307251\n",
      "average propensity for treated: 0.5288442969322205 and untreated: 0.5137276649475098\n",
      "average propensity for treated: 0.5288442969322205 and untreated: 0.5137276649475098\n",
      "average propensity for treated: 0.5203579664230347 and untreated: 0.530987024307251\n",
      "Simulation 3 results:\n",
      "  Test err: 0.0611\n",
      "  Test dr_err: 0.5905\n",
      "  Test ipw_error: 1.0756\n",
      "\n",
      "Processing file 5/30: ihdp_shifted_14.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 41.53535842895508, validation -- 352.5997619628906, epoch 8\n",
      "SGD loss: train --  39.73419189453125, validation -- 349.7677001953125,  epoch 56\n",
      "***************************** elapsed_time is:  1.8860995769500732\n",
      "average propensity for treated: 0.5059179663658142 and untreated: 0.5037391781806946\n",
      "average propensity for treated: 0.5059179663658142 and untreated: 0.5037391781806946\n",
      "average propensity for treated: 0.5059179663658142 and untreated: 0.5037391781806946\n",
      "average propensity for treated: 0.5059179663658142 and untreated: 0.5037391781806946\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.182818412780762, validation -- 98.26383972167969, epoch 5\n",
      "SGD loss: train --  6.645366668701172, validation -- 97.14236450195312,  epoch 99\n",
      "***************************** elapsed_time is:  5.814743280410767\n",
      "average propensity for treated: 0.49947622418403625 and untreated: 0.4960750341415405\n",
      "average propensity for treated: 0.49309131503105164 and untreated: 0.49448201060295105\n",
      "average propensity for treated: 0.49309131503105164 and untreated: 0.49448201060295105\n",
      "average propensity for treated: 0.49947622418403625 and untreated: 0.4960750341415405\n",
      "Simulation 4 results:\n",
      "  Test err: 1.5616\n",
      "  Test dr_err: 1.2092\n",
      "  Test ipw_error: 2.0378\n",
      "\n",
      "Processing file 6/30: ihdp_shifted_15.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 39.664852142333984, validation -- 351.023193359375, epoch 9\n",
      "SGD loss: train --  37.62255096435547, validation -- 338.95037841796875,  epoch 99\n",
      "***************************** elapsed_time is:  3.2204713821411133\n",
      "average propensity for treated: 0.5011555552482605 and untreated: 0.49687477946281433\n",
      "average propensity for treated: 0.5011555552482605 and untreated: 0.49687477946281433\n",
      "average propensity for treated: 0.5011555552482605 and untreated: 0.49687477946281433\n",
      "average propensity for treated: 0.5011555552482605 and untreated: 0.49687477946281433\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.617389678955078, validation -- 84.52735137939453, epoch 4\n",
      "SGD loss: train --  6.678810119628906, validation -- 83.66239929199219,  epoch 99\n",
      "***************************** elapsed_time is:  5.600354194641113\n",
      "average propensity for treated: 0.4811890721321106 and untreated: 0.4824836254119873\n",
      "average propensity for treated: 0.48615121841430664 and untreated: 0.48139816522598267\n",
      "average propensity for treated: 0.48615121841430664 and untreated: 0.48139816522598267\n",
      "average propensity for treated: 0.4811890721321106 and untreated: 0.4824836254119873\n",
      "Simulation 5 results:\n",
      "  Test err: 0.6469\n",
      "  Test dr_err: 0.8376\n",
      "  Test ipw_error: 0.9890\n",
      "\n",
      "Processing file 7/30: ihdp_shifted_16.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 40.78652572631836, validation -- 362.43994140625, epoch 7\n",
      "SGD loss: train --  37.62224578857422, validation -- 347.01776123046875,  epoch 99\n",
      "***************************** elapsed_time is:  3.147101640701294\n",
      "average propensity for treated: 0.443267822265625 and untreated: 0.4407449960708618\n",
      "average propensity for treated: 0.443267822265625 and untreated: 0.4407449960708618\n",
      "average propensity for treated: 0.443267822265625 and untreated: 0.4407449960708618\n",
      "average propensity for treated: 0.443267822265625 and untreated: 0.4407449960708618\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 8.690725326538086, validation -- 99.74217224121094, epoch 4\n",
      "SGD loss: train --  7.760224342346191, validation -- 98.65217590332031,  epoch 94\n",
      "***************************** elapsed_time is:  5.439407825469971\n",
      "average propensity for treated: 0.4700981080532074 and untreated: 0.47569960355758667\n",
      "average propensity for treated: 0.4675469398498535 and untreated: 0.46201151609420776\n",
      "average propensity for treated: 0.4675469398498535 and untreated: 0.46201151609420776\n",
      "average propensity for treated: 0.4700981080532074 and untreated: 0.47569960355758667\n",
      "Simulation 6 results:\n",
      "  Test err: 0.7907\n",
      "  Test dr_err: 1.2419\n",
      "  Test ipw_error: 0.7699\n",
      "\n",
      "Processing file 8/30: ihdp_shifted_17.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 40.519100189208984, validation -- 311.60009765625, epoch 12\n",
      "SGD loss: train --  39.413089752197266, validation -- 310.60595703125,  epoch 60\n",
      "***************************** elapsed_time is:  2.1562063694000244\n",
      "average propensity for treated: 0.49229496717453003 and untreated: 0.494836688041687\n",
      "average propensity for treated: 0.49229496717453003 and untreated: 0.494836688041687\n",
      "average propensity for treated: 0.49229496717453003 and untreated: 0.494836688041687\n",
      "average propensity for treated: 0.49229496717453003 and untreated: 0.494836688041687\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.056395530700684, validation -- 82.89132690429688, epoch 4\n",
      "SGD loss: train --  6.290752410888672, validation -- 82.42230987548828,  epoch 99\n",
      "***************************** elapsed_time is:  5.7104175090789795\n",
      "average propensity for treated: 0.5085644125938416 and untreated: 0.5183210372924805\n",
      "average propensity for treated: 0.5140312314033508 and untreated: 0.5109831690788269\n",
      "average propensity for treated: 0.5140312314033508 and untreated: 0.5109831690788269\n",
      "average propensity for treated: 0.5085644125938416 and untreated: 0.5183210372924805\n",
      "Simulation 7 results:\n",
      "  Test err: 0.8094\n",
      "  Test dr_err: 1.2730\n",
      "  Test ipw_error: 1.0752\n",
      "\n",
      "Processing file 9/30: ihdp_shifted_18.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 45.8342170715332, validation -- 352.312255859375, epoch 5\n",
      "SGD loss: train --  41.885318756103516, validation -- 338.691162109375,  epoch 99\n",
      "***************************** elapsed_time is:  3.125012159347534\n",
      "average propensity for treated: 0.45580604672431946 and untreated: 0.452047199010849\n",
      "average propensity for treated: 0.45580604672431946 and untreated: 0.452047199010849\n",
      "average propensity for treated: 0.45580604672431946 and untreated: 0.452047199010849\n",
      "average propensity for treated: 0.45580604672431946 and untreated: 0.452047199010849\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 9.332014083862305, validation -- 98.7681655883789, epoch 3\n",
      "SGD loss: train --  8.82194709777832, validation -- 97.27394104003906,  epoch 67\n",
      "***************************** elapsed_time is:  3.9704692363739014\n",
      "average propensity for treated: 0.4385085105895996 and untreated: 0.4408024847507477\n",
      "average propensity for treated: 0.4392628073692322 and untreated: 0.4364934265613556\n",
      "average propensity for treated: 0.4392628073692322 and untreated: 0.4364934265613556\n",
      "average propensity for treated: 0.4385085105895996 and untreated: 0.4408024847507477\n",
      "Simulation 8 results:\n",
      "  Test err: 1.5738\n",
      "  Test dr_err: 0.1708\n",
      "  Test ipw_error: 1.7543\n",
      "\n",
      "Processing file 10/30: ihdp_shifted_19.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 40.75590133666992, validation -- 352.99090576171875, epoch 12\n",
      "SGD loss: train --  38.867454528808594, validation -- 349.9520568847656,  epoch 99\n",
      "***************************** elapsed_time is:  3.188906669616699\n",
      "average propensity for treated: 0.5072568655014038 and untreated: 0.49520304799079895\n",
      "average propensity for treated: 0.5072568655014038 and untreated: 0.49520304799079895\n",
      "average propensity for treated: 0.5072568655014038 and untreated: 0.49520304799079895\n",
      "average propensity for treated: 0.5072568655014038 and untreated: 0.49520304799079895\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.934870719909668, validation -- 84.25662994384766, epoch 5\n",
      "SGD loss: train --  6.7032470703125, validation -- 84.63871765136719,  epoch 40\n",
      "***************************** elapsed_time is:  2.5503172874450684\n",
      "average propensity for treated: 0.5154853463172913 and untreated: 0.49814465641975403\n",
      "average propensity for treated: 0.5055627822875977 and untreated: 0.501068651676178\n",
      "average propensity for treated: 0.5055627822875977 and untreated: 0.501068651676178\n",
      "average propensity for treated: 0.5154853463172913 and untreated: 0.49814465641975403\n",
      "Simulation 9 results:\n",
      "  Test err: 0.9663\n",
      "  Test dr_err: 0.6485\n",
      "  Test ipw_error: 0.2989\n",
      "\n",
      "Processing file 11/30: ihdp_shifted_2.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 46.10953140258789, validation -- 327.1199951171875, epoch 8\n",
      "SGD loss: train --  44.139583587646484, validation -- 326.8760986328125,  epoch 49\n",
      "***************************** elapsed_time is:  1.7032804489135742\n",
      "average propensity for treated: 0.5046491026878357 and untreated: 0.49896612763404846\n",
      "average propensity for treated: 0.5046491026878357 and untreated: 0.49896612763404846\n",
      "average propensity for treated: 0.5046491026878357 and untreated: 0.49896612763404846\n",
      "average propensity for treated: 0.5046491026878357 and untreated: 0.49896612763404846\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 9.151741027832031, validation -- 120.66805267333984, epoch 3\n",
      "SGD loss: train --  8.456945419311523, validation -- 119.58773040771484,  epoch 63\n",
      "***************************** elapsed_time is:  3.591095209121704\n",
      "average propensity for treated: 0.4931408166885376 and untreated: 0.4840935170650482\n",
      "average propensity for treated: 0.48347434401512146 and untreated: 0.482745885848999\n",
      "average propensity for treated: 0.48347434401512146 and untreated: 0.482745885848999\n",
      "average propensity for treated: 0.4931408166885376 and untreated: 0.4840935170650482\n",
      "Simulation 10 results:\n",
      "  Test err: 0.0880\n",
      "  Test dr_err: 0.5901\n",
      "  Test ipw_error: 1.0060\n",
      "\n",
      "Processing file 12/30: ihdp_shifted_20.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 41.9749755859375, validation -- 342.1861267089844, epoch 8\n",
      "SGD loss: train --  40.41082000732422, validation -- 341.3062744140625,  epoch 62\n",
      "***************************** elapsed_time is:  2.0767605304718018\n",
      "average propensity for treated: 0.5136991143226624 and untreated: 0.5068645477294922\n",
      "average propensity for treated: 0.5136991143226624 and untreated: 0.5068645477294922\n",
      "average propensity for treated: 0.5136991143226624 and untreated: 0.5068645477294922\n",
      "average propensity for treated: 0.5136991143226624 and untreated: 0.5068645477294922\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.365522384643555, validation -- 91.33811950683594, epoch 15\n",
      "SGD loss: train --  6.045388698577881, validation -- 90.81038665771484,  epoch 99\n",
      "***************************** elapsed_time is:  6.615352392196655\n",
      "average propensity for treated: 0.4632594585418701 and untreated: 0.48267045617103577\n",
      "average propensity for treated: 0.48650434613227844 and untreated: 0.4698250889778137\n",
      "average propensity for treated: 0.48650434613227844 and untreated: 0.4698250889778137\n",
      "average propensity for treated: 0.4632594585418701 and untreated: 0.48267045617103577\n",
      "Simulation 11 results:\n",
      "  Test err: 0.7470\n",
      "  Test dr_err: 1.0594\n",
      "  Test ipw_error: 1.7960\n",
      "\n",
      "Processing file 13/30: ihdp_shifted_21.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 41.93090057373047, validation -- 344.8446960449219, epoch 8\n",
      "SGD loss: train --  39.294010162353516, validation -- 336.9080810546875,  epoch 99\n",
      "***************************** elapsed_time is:  3.1384313106536865\n",
      "average propensity for treated: 0.5061708688735962 and untreated: 0.5022790431976318\n",
      "average propensity for treated: 0.5061708688735962 and untreated: 0.5022790431976318\n",
      "average propensity for treated: 0.5061708688735962 and untreated: 0.5022790431976318\n",
      "average propensity for treated: 0.5061708688735962 and untreated: 0.5022790431976318\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.114376068115234, validation -- 95.56566619873047, epoch 4\n",
      "SGD loss: train --  6.640142917633057, validation -- 94.37342834472656,  epoch 84\n",
      "***************************** elapsed_time is:  5.265416622161865\n",
      "average propensity for treated: 0.44614243507385254 and untreated: 0.45480838418006897\n",
      "average propensity for treated: 0.45532089471817017 and untreated: 0.4526004195213318\n",
      "average propensity for treated: 0.45532089471817017 and untreated: 0.4526004195213318\n",
      "average propensity for treated: 0.44614243507385254 and untreated: 0.45480838418006897\n",
      "Simulation 12 results:\n",
      "  Test err: 1.9338\n",
      "  Test dr_err: 1.0388\n",
      "  Test ipw_error: 0.2886\n",
      "\n",
      "Processing file 14/30: ihdp_shifted_22.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 43.656314849853516, validation -- 355.29718017578125, epoch 9\n",
      "SGD loss: train --  40.943119049072266, validation -- 351.1317138671875,  epoch 81\n",
      "***************************** elapsed_time is:  3.0144340991973877\n",
      "average propensity for treated: 0.44757455587387085 and untreated: 0.4449063837528229\n",
      "average propensity for treated: 0.44757455587387085 and untreated: 0.4449063837528229\n",
      "average propensity for treated: 0.44757455587387085 and untreated: 0.4449063837528229\n",
      "average propensity for treated: 0.44757455587387085 and untreated: 0.4449063837528229\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.21502161026001, validation -- 99.55018615722656, epoch 3\n",
      "SGD loss: train --  6.641024589538574, validation -- 99.16814422607422,  epoch 80\n",
      "***************************** elapsed_time is:  4.749798536300659\n",
      "average propensity for treated: 0.42134028673171997 and untreated: 0.42254966497421265\n",
      "average propensity for treated: 0.44161316752433777 and untreated: 0.417492538690567\n",
      "average propensity for treated: 0.44161316752433777 and untreated: 0.417492538690567\n",
      "average propensity for treated: 0.42134028673171997 and untreated: 0.42254966497421265\n",
      "Simulation 13 results:\n",
      "  Test err: 0.4520\n",
      "  Test dr_err: 1.0564\n",
      "  Test ipw_error: 1.7958\n",
      "\n",
      "Processing file 15/30: ihdp_shifted_23.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 48.00645065307617, validation -- 341.104736328125, epoch 6\n",
      "SGD loss: train --  43.52174377441406, validation -- 328.0452880859375,  epoch 99\n",
      "***************************** elapsed_time is:  2.999606132507324\n",
      "average propensity for treated: 0.5032892227172852 and untreated: 0.5033189654350281\n",
      "average propensity for treated: 0.5032892227172852 and untreated: 0.5033189654350281\n",
      "average propensity for treated: 0.5032892227172852 and untreated: 0.5033189654350281\n",
      "average propensity for treated: 0.5032892227172852 and untreated: 0.5033189654350281\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.733547210693359, validation -- 100.00640106201172, epoch 5\n",
      "SGD loss: train --  6.205137729644775, validation -- 99.72654724121094,  epoch 99\n",
      "***************************** elapsed_time is:  6.287952899932861\n",
      "average propensity for treated: 0.5207014679908752 and untreated: 0.5056952834129333\n",
      "average propensity for treated: 0.500488817691803 and untreated: 0.48154962062835693\n",
      "average propensity for treated: 0.500488817691803 and untreated: 0.48154962062835693\n",
      "average propensity for treated: 0.5207014679908752 and untreated: 0.5056952834129333\n",
      "Simulation 14 results:\n",
      "  Test err: 0.8279\n",
      "  Test dr_err: 0.6430\n",
      "  Test ipw_error: 0.3801\n",
      "\n",
      "Processing file 16/30: ihdp_shifted_24.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 40.422786712646484, validation -- 333.84503173828125, epoch 8\n",
      "SGD loss: train --  38.59225845336914, validation -- 324.63726806640625,  epoch 86\n",
      "***************************** elapsed_time is:  3.217740297317505\n",
      "average propensity for treated: 0.5286427140235901 and untreated: 0.5266337990760803\n",
      "average propensity for treated: 0.5286427140235901 and untreated: 0.5266337990760803\n",
      "average propensity for treated: 0.5286427140235901 and untreated: 0.5266337990760803\n",
      "average propensity for treated: 0.5286427140235901 and untreated: 0.5266337990760803\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.130723476409912, validation -- 102.4544448852539, epoch 4\n",
      "SGD loss: train --  6.893959045410156, validation -- 102.64511108398438,  epoch 40\n",
      "***************************** elapsed_time is:  2.854283332824707\n",
      "average propensity for treated: 0.5301087498664856 and untreated: 0.5197457075119019\n",
      "average propensity for treated: 0.5060933828353882 and untreated: 0.524889349937439\n",
      "average propensity for treated: 0.5060933828353882 and untreated: 0.524889349937439\n",
      "average propensity for treated: 0.5301087498664856 and untreated: 0.5197457075119019\n",
      "Simulation 15 results:\n",
      "  Test err: 0.0602\n",
      "  Test dr_err: 0.4636\n",
      "  Test ipw_error: 0.7326\n",
      "\n",
      "Processing file 17/30: ihdp_shifted_25.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 42.760833740234375, validation -- 354.38916015625, epoch 7\n",
      "SGD loss: train --  40.86833953857422, validation -- 345.25250244140625,  epoch 58\n",
      "***************************** elapsed_time is:  2.0736899375915527\n",
      "average propensity for treated: 0.5019113421440125 and untreated: 0.4948308765888214\n",
      "average propensity for treated: 0.5019113421440125 and untreated: 0.4948308765888214\n",
      "average propensity for treated: 0.5019113421440125 and untreated: 0.4948308765888214\n",
      "average propensity for treated: 0.5019113421440125 and untreated: 0.4948308765888214\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.3648858070373535, validation -- 101.89636993408203, epoch 6\n",
      "SGD loss: train --  6.819160461425781, validation -- 101.63624572753906,  epoch 78\n",
      "***************************** elapsed_time is:  5.019550800323486\n",
      "average propensity for treated: 0.46410468220710754 and untreated: 0.4582184851169586\n",
      "average propensity for treated: 0.4736163020133972 and untreated: 0.4606533944606781\n",
      "average propensity for treated: 0.4736163020133972 and untreated: 0.4606533944606781\n",
      "average propensity for treated: 0.46410468220710754 and untreated: 0.4582184851169586\n",
      "Simulation 16 results:\n",
      "  Test err: 1.2194\n",
      "  Test dr_err: 1.3808\n",
      "  Test ipw_error: 1.8954\n",
      "\n",
      "Processing file 18/30: ihdp_shifted_26.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 44.79227828979492, validation -- 365.55340576171875, epoch 5\n",
      "SGD loss: train --  41.949642181396484, validation -- 355.1566162109375,  epoch 78\n",
      "***************************** elapsed_time is:  2.7961206436157227\n",
      "average propensity for treated: 0.5029718279838562 and untreated: 0.49580079317092896\n",
      "average propensity for treated: 0.5029718279838562 and untreated: 0.49580079317092896\n",
      "average propensity for treated: 0.5029718279838562 and untreated: 0.49580079317092896\n",
      "average propensity for treated: 0.5029718279838562 and untreated: 0.49580079317092896\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.887876987457275, validation -- 86.84519958496094, epoch 4\n",
      "SGD loss: train --  6.6012163162231445, validation -- 87.3172607421875,  epoch 40\n",
      "***************************** elapsed_time is:  2.7673463821411133\n",
      "average propensity for treated: 0.44645771384239197 and untreated: 0.45695093274116516\n",
      "average propensity for treated: 0.4613134264945984 and untreated: 0.449368953704834\n",
      "average propensity for treated: 0.4613134264945984 and untreated: 0.449368953704834\n",
      "average propensity for treated: 0.44645771384239197 and untreated: 0.45695093274116516\n",
      "Simulation 17 results:\n",
      "  Test err: 0.1811\n",
      "  Test dr_err: 1.3994\n",
      "  Test ipw_error: 2.7339\n",
      "\n",
      "Processing file 19/30: ihdp_shifted_27.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 40.427955627441406, validation -- 339.5272521972656, epoch 8\n",
      "SGD loss: train --  38.636844635009766, validation -- 338.1917419433594,  epoch 80\n",
      "***************************** elapsed_time is:  2.9190142154693604\n",
      "average propensity for treated: 0.5597553253173828 and untreated: 0.557464599609375\n",
      "average propensity for treated: 0.5597553253173828 and untreated: 0.557464599609375\n",
      "average propensity for treated: 0.5597553253173828 and untreated: 0.557464599609375\n",
      "average propensity for treated: 0.5597553253173828 and untreated: 0.557464599609375\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.444737434387207, validation -- 83.95487213134766, epoch 10\n",
      "SGD loss: train --  6.2793288230896, validation -- 84.03966522216797,  epoch 40\n",
      "***************************** elapsed_time is:  3.287548542022705\n",
      "average propensity for treated: 0.4900917112827301 and untreated: 0.4924679100513458\n",
      "average propensity for treated: 0.4930952787399292 and untreated: 0.49625369906425476\n",
      "average propensity for treated: 0.4930952787399292 and untreated: 0.49625369906425476\n",
      "average propensity for treated: 0.4900917112827301 and untreated: 0.4924679100513458\n",
      "Simulation 18 results:\n",
      "  Test err: 0.5865\n",
      "  Test dr_err: 1.3780\n",
      "  Test ipw_error: 1.7859\n",
      "\n",
      "Processing file 20/30: ihdp_shifted_28.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 42.59153747558594, validation -- 355.60528564453125, epoch 8\n",
      "SGD loss: train --  40.79802703857422, validation -- 348.48480224609375,  epoch 99\n",
      "***************************** elapsed_time is:  3.2773594856262207\n",
      "average propensity for treated: 0.5467891097068787 and untreated: 0.541054368019104\n",
      "average propensity for treated: 0.5467891097068787 and untreated: 0.541054368019104\n",
      "average propensity for treated: 0.5467891097068787 and untreated: 0.541054368019104\n",
      "average propensity for treated: 0.5467891097068787 and untreated: 0.541054368019104\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 8.44603157043457, validation -- 99.45392608642578, epoch 2\n",
      "SGD loss: train --  7.34488582611084, validation -- 95.30171966552734,  epoch 99\n",
      "***************************** elapsed_time is:  6.17189621925354\n",
      "average propensity for treated: 0.5457918643951416 and untreated: 0.5377837419509888\n",
      "average propensity for treated: 0.543433427810669 and untreated: 0.5338661670684814\n",
      "average propensity for treated: 0.543433427810669 and untreated: 0.5338661670684814\n",
      "average propensity for treated: 0.5457918643951416 and untreated: 0.5377837419509888\n",
      "Simulation 19 results:\n",
      "  Test err: 0.2130\n",
      "  Test dr_err: 0.6457\n",
      "  Test ipw_error: 1.3051\n",
      "\n",
      "Processing file 21/30: ihdp_shifted_30.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 42.074886322021484, validation -- 344.3788146972656, epoch 8\n",
      "SGD loss: train --  40.00412368774414, validation -- 334.754150390625,  epoch 99\n",
      "***************************** elapsed_time is:  3.384765386581421\n",
      "average propensity for treated: 0.5040709972381592 and untreated: 0.49595001339912415\n",
      "average propensity for treated: 0.5040709972381592 and untreated: 0.49595001339912415\n",
      "average propensity for treated: 0.5040709972381592 and untreated: 0.49595001339912415\n",
      "average propensity for treated: 0.5040709972381592 and untreated: 0.49595001339912415\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.313941955566406, validation -- 89.8975601196289, epoch 6\n",
      "SGD loss: train --  6.799503326416016, validation -- 89.32124328613281,  epoch 99\n",
      "***************************** elapsed_time is:  7.051783084869385\n",
      "average propensity for treated: 0.4704877734184265 and untreated: 0.4735693633556366\n",
      "average propensity for treated: 0.457661896944046 and untreated: 0.45560526847839355\n",
      "average propensity for treated: 0.457661896944046 and untreated: 0.45560526847839355\n",
      "average propensity for treated: 0.4704877734184265 and untreated: 0.4735693633556366\n",
      "Simulation 20 results:\n",
      "  Test err: 0.0125\n",
      "  Test dr_err: 0.5909\n",
      "  Test ipw_error: 0.1800\n",
      "\n",
      "Processing file 22/30: ihdp_shifted_31.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 38.92111587524414, validation -- 349.560791015625, epoch 14\n",
      "SGD loss: train --  37.88862228393555, validation -- 347.6612854003906,  epoch 99\n",
      "***************************** elapsed_time is:  4.107269763946533\n",
      "average propensity for treated: 0.5350413918495178 and untreated: 0.5396443605422974\n",
      "average propensity for treated: 0.5350413918495178 and untreated: 0.5396443605422974\n",
      "average propensity for treated: 0.5350413918495178 and untreated: 0.5396443605422974\n",
      "average propensity for treated: 0.5350413918495178 and untreated: 0.5396443605422974\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 8.7388334274292, validation -- 118.65129852294922, epoch 4\n",
      "SGD loss: train --  7.759971618652344, validation -- 110.86581420898438,  epoch 99\n",
      "***************************** elapsed_time is:  6.941634178161621\n",
      "average propensity for treated: 0.5720905661582947 and untreated: 0.570052981376648\n",
      "average propensity for treated: 0.5632529854774475 and untreated: 0.5640333890914917\n",
      "average propensity for treated: 0.5632529854774475 and untreated: 0.5640333890914917\n",
      "average propensity for treated: 0.5720905661582947 and untreated: 0.570052981376648\n",
      "Simulation 21 results:\n",
      "  Test err: 0.5552\n",
      "  Test dr_err: 1.4720\n",
      "  Test ipw_error: 0.0623\n",
      "\n",
      "Processing file 23/30: ihdp_shifted_32.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 39.96720886230469, validation -- 344.428466796875, epoch 11\n",
      "SGD loss: train --  38.86607360839844, validation -- 338.706298828125,  epoch 89\n",
      "***************************** elapsed_time is:  3.9266159534454346\n",
      "average propensity for treated: 0.5076751708984375 and untreated: 0.5026307702064514\n",
      "average propensity for treated: 0.5076751708984375 and untreated: 0.5026307702064514\n",
      "average propensity for treated: 0.5076751708984375 and untreated: 0.5026307702064514\n",
      "average propensity for treated: 0.5076751708984375 and untreated: 0.5026307702064514\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.909882545471191, validation -- 92.34403228759766, epoch 5\n",
      "SGD loss: train --  6.708186149597168, validation -- 92.41117095947266,  epoch 40\n",
      "***************************** elapsed_time is:  2.794360399246216\n",
      "average propensity for treated: 0.4845293462276459 and untreated: 0.48600244522094727\n",
      "average propensity for treated: 0.4915074408054352 and untreated: 0.4785996079444885\n",
      "average propensity for treated: 0.4915074408054352 and untreated: 0.4785996079444885\n",
      "average propensity for treated: 0.4845293462276459 and untreated: 0.48600244522094727\n",
      "Simulation 22 results:\n",
      "  Test err: 1.2437\n",
      "  Test dr_err: 1.0930\n",
      "  Test ipw_error: 1.5713\n",
      "\n",
      "Processing file 24/30: ihdp_shifted_33.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 42.68608093261719, validation -- 329.76934814453125, epoch 8\n",
      "SGD loss: train --  40.17076873779297, validation -- 319.77520751953125,  epoch 99\n",
      "***************************** elapsed_time is:  3.510014533996582\n",
      "average propensity for treated: 0.522968590259552 and untreated: 0.5127662420272827\n",
      "average propensity for treated: 0.522968590259552 and untreated: 0.5127662420272827\n",
      "average propensity for treated: 0.522968590259552 and untreated: 0.5127662420272827\n",
      "average propensity for treated: 0.522968590259552 and untreated: 0.5127662420272827\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 7.32940149307251, validation -- 104.48876953125, epoch 6\n",
      "SGD loss: train --  6.9541144371032715, validation -- 104.37947845458984,  epoch 54\n",
      "***************************** elapsed_time is:  3.524143695831299\n",
      "average propensity for treated: 0.4762571454048157 and untreated: 0.4842393398284912\n",
      "average propensity for treated: 0.5005704164505005 and untreated: 0.4686730206012726\n",
      "average propensity for treated: 0.5005704164505005 and untreated: 0.4686730206012726\n",
      "average propensity for treated: 0.4762571454048157 and untreated: 0.4842393398284912\n",
      "Simulation 23 results:\n",
      "  Test err: 0.5668\n",
      "  Test dr_err: 1.4455\n",
      "  Test ipw_error: 0.8823\n",
      "\n",
      "Processing file 25/30: ihdp_shifted_4.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 38.35126495361328, validation -- 339.5459289550781, epoch 10\n",
      "SGD loss: train --  36.946693420410156, validation -- 334.9124450683594,  epoch 99\n",
      "***************************** elapsed_time is:  3.4411468505859375\n",
      "average propensity for treated: 0.5274801850318909 and untreated: 0.5206860303878784\n",
      "average propensity for treated: 0.5274801850318909 and untreated: 0.5206860303878784\n",
      "average propensity for treated: 0.5274801850318909 and untreated: 0.5206860303878784\n",
      "average propensity for treated: 0.5274801850318909 and untreated: 0.5206860303878784\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 8.261474609375, validation -- 113.9471435546875, epoch 2\n",
      "SGD loss: train --  7.537741184234619, validation -- 101.57470703125,  epoch 66\n",
      "***************************** elapsed_time is:  4.187420845031738\n",
      "average propensity for treated: 0.5084723830223083 and untreated: 0.49566084146499634\n",
      "average propensity for treated: 0.49010905623435974 and untreated: 0.49807658791542053\n",
      "average propensity for treated: 0.49010905623435974 and untreated: 0.49807658791542053\n",
      "average propensity for treated: 0.5084723830223083 and untreated: 0.49566084146499634\n",
      "Simulation 24 results:\n",
      "  Test err: 0.6296\n",
      "  Test dr_err: 1.7929\n",
      "  Test ipw_error: 2.2225\n",
      "\n",
      "Processing file 26/30: ihdp_shifted_5.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 43.37017822265625, validation -- 365.48712158203125, epoch 7\n",
      "SGD loss: train --  41.08980178833008, validation -- 350.81378173828125,  epoch 99\n",
      "***************************** elapsed_time is:  3.474440813064575\n",
      "average propensity for treated: 0.5115565061569214 and untreated: 0.5053349733352661\n",
      "average propensity for treated: 0.5115565061569214 and untreated: 0.5053349733352661\n",
      "average propensity for treated: 0.5115565061569214 and untreated: 0.5053349733352661\n",
      "average propensity for treated: 0.5115565061569214 and untreated: 0.5053349733352661\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 8.557355880737305, validation -- 92.68000030517578, epoch 2\n",
      "SGD loss: train --  7.295321464538574, validation -- 89.25579833984375,  epoch 99\n",
      "***************************** elapsed_time is:  6.174064636230469\n",
      "average propensity for treated: 0.4963676631450653 and untreated: 0.49836769700050354\n",
      "average propensity for treated: 0.5110812783241272 and untreated: 0.49346795678138733\n",
      "average propensity for treated: 0.5110812783241272 and untreated: 0.49346795678138733\n",
      "average propensity for treated: 0.4963676631450653 and untreated: 0.49836769700050354\n",
      "Simulation 25 results:\n",
      "  Test err: 0.8169\n",
      "  Test dr_err: 0.9030\n",
      "  Test ipw_error: 0.5400\n",
      "\n",
      "Processing file 27/30: ihdp_shifted_6.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 42.809505462646484, validation -- 343.8179016113281, epoch 8\n",
      "SGD loss: train --  40.43608093261719, validation -- 332.3396301269531,  epoch 81\n",
      "***************************** elapsed_time is:  3.2213492393493652\n",
      "average propensity for treated: 0.4818703532218933 and untreated: 0.4836284816265106\n",
      "average propensity for treated: 0.4818703532218933 and untreated: 0.4836284816265106\n",
      "average propensity for treated: 0.4818703532218933 and untreated: 0.4836284816265106\n",
      "average propensity for treated: 0.4818703532218933 and untreated: 0.4836284816265106\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 8.474512100219727, validation -- 102.57503509521484, epoch 4\n",
      "SGD loss: train --  8.090795516967773, validation -- 102.75237274169922,  epoch 40\n",
      "***************************** elapsed_time is:  3.2155609130859375\n",
      "average propensity for treated: 0.5049036145210266 and untreated: 0.49623945355415344\n",
      "average propensity for treated: 0.48176202178001404 and untreated: 0.49354228377342224\n",
      "average propensity for treated: 0.48176202178001404 and untreated: 0.49354228377342224\n",
      "average propensity for treated: 0.5049036145210266 and untreated: 0.49623945355415344\n",
      "Simulation 26 results:\n",
      "  Test err: 1.5285\n",
      "  Test dr_err: 1.2883\n",
      "  Test ipw_error: 0.5456\n",
      "\n",
      "Processing file 28/30: ihdp_shifted_7.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 44.37630081176758, validation -- 336.6898193359375, epoch 8\n",
      "SGD loss: train --  41.43833923339844, validation -- 327.42694091796875,  epoch 67\n",
      "***************************** elapsed_time is:  2.8322672843933105\n",
      "average propensity for treated: 0.4883454144001007 and untreated: 0.4857266843318939\n",
      "average propensity for treated: 0.4883454144001007 and untreated: 0.4857266843318939\n",
      "average propensity for treated: 0.4883454144001007 and untreated: 0.4857266843318939\n",
      "average propensity for treated: 0.4883454144001007 and untreated: 0.4857266843318939\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 8.660370826721191, validation -- 105.54945373535156, epoch 2\n",
      "SGD loss: train --  8.004459381103516, validation -- 105.95518493652344,  epoch 40\n",
      "***************************** elapsed_time is:  2.6438610553741455\n",
      "average propensity for treated: 0.4776906967163086 and untreated: 0.4696882665157318\n",
      "average propensity for treated: 0.4636194705963135 and untreated: 0.4521239101886749\n",
      "average propensity for treated: 0.4636194705963135 and untreated: 0.4521239101886749\n",
      "average propensity for treated: 0.4776906967163086 and untreated: 0.4696882665157318\n",
      "Simulation 27 results:\n",
      "  Test err: 0.3067\n",
      "  Test dr_err: 0.8924\n",
      "  Test ipw_error: 0.4006\n",
      "\n",
      "Processing file 29/30: ihdp_shifted_8.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 42.939266204833984, validation -- 318.966796875, epoch 8\n",
      "SGD loss: train --  41.01205825805664, validation -- 318.10101318359375,  epoch 64\n",
      "***************************** elapsed_time is:  2.579620361328125\n",
      "average propensity for treated: 0.5089378356933594 and untreated: 0.5060778856277466\n",
      "average propensity for treated: 0.5089378356933594 and untreated: 0.5060778856277466\n",
      "average propensity for treated: 0.5089378356933594 and untreated: 0.5060778856277466\n",
      "average propensity for treated: 0.5089378356933594 and untreated: 0.5060778856277466\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 8.592706680297852, validation -- 119.25658416748047, epoch 2\n",
      "SGD loss: train --  7.603964328765869, validation -- 109.43757629394531,  epoch 99\n",
      "***************************** elapsed_time is:  5.804811000823975\n",
      "average propensity for treated: 0.5298107266426086 and untreated: 0.5268095135688782\n",
      "average propensity for treated: 0.5224399566650391 and untreated: 0.5298724174499512\n",
      "average propensity for treated: 0.5224399566650391 and untreated: 0.5298724174499512\n",
      "average propensity for treated: 0.5298107266426086 and untreated: 0.5268095135688782\n",
      "Simulation 28 results:\n",
      "  Test err: 1.1427\n",
      "  Test dr_err: 0.9928\n",
      "  Test ipw_error: 1.0892\n",
      "\n",
      "Processing file 30/30: ihdp_shifted_9.csv\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.8464e+02 -1.0690e+04  4e+04  5e-02  8e-13\n",
      " 1:  2.6162e+02 -9.4791e+03  3e+04  4e-02  6e-13\n",
      " 2:  2.7142e+02 -3.2144e+03  3e+03  2e-16  5e-13\n",
      " 3:  2.7120e+02  1.5325e+00  3e+02  1e-16  9e-14\n",
      " 4:  2.6761e+02  5.7153e+01  2e+02  2e-16  4e-14\n",
      " 5:  2.6605e+02  2.1055e+02  6e+01  5e-16  3e-12\n",
      " 6:  2.6237e+02  2.4206e+02  2e+01  1e-16  4e-12\n",
      " 7:  2.6030e+02  2.5587e+02  4e+00  3e-16  1e-12\n",
      " 8:  2.5933e+02  2.5862e+02  7e-01  2e-16  2e-12\n",
      " 9:  2.5910e+02  2.5904e+02  5e-02  1e-16  4e-12\n",
      "10:  2.5907e+02  2.5907e+02  1e-03  4e-16  1e-11\n",
      "11:  2.5907e+02  2.5907e+02  2e-05  8e-16  1e-10\n",
      "Optimal solution found.\n",
      "Is targeted regularization: False\n",
      "Creating TarNet model\n",
      "Adam loss: train -- 42.920040130615234, validation -- 354.6067199707031, epoch 8\n",
      "SGD loss: train --  39.59983444213867, validation -- 344.28118896484375,  epoch 99\n",
      "***************************** elapsed_time is:  3.4912049770355225\n",
      "average propensity for treated: 0.45266473293304443 and untreated: 0.4536631405353546\n",
      "average propensity for treated: 0.45266473293304443 and untreated: 0.4536631405353546\n",
      "average propensity for treated: 0.45266473293304443 and untreated: 0.4536631405353546\n",
      "average propensity for treated: 0.45266473293304443 and untreated: 0.4536631405353546\n",
      "Creating TarNet_transfer model\n",
      "Adam loss: train -- 6.647862911224365, validation -- 93.77289581298828, epoch 6\n",
      "SGD loss: train --  6.4664764404296875, validation -- 93.81080627441406,  epoch 40\n",
      "***************************** elapsed_time is:  2.9075560569763184\n",
      "average propensity for treated: 0.47887271642684937 and untreated: 0.48619556427001953\n",
      "average propensity for treated: 0.473136305809021 and untreated: 0.47759899497032166\n",
      "average propensity for treated: 0.473136305809021 and untreated: 0.47759899497032166\n",
      "average propensity for treated: 0.47887271642684937 and untreated: 0.48619556427001953\n",
      "Simulation 29 results:\n",
      "  Test err: 0.4928\n",
      "  Test dr_err: 1.5672\n",
      "  Test ipw_error: 2.2015\n",
      "\n",
      "==============================\n",
      "Error Summary for All Simulations:\n",
      "Successful runs: 30/30\n",
      "ATE Error: Mean = 0.7977, Variance = 0.2664\n",
      "DR Error: Mean = 1.0219, Variance = 0.1403\n",
      "IPW Error: Mean = 1.1388, Variance = 0.4783\n",
      "Results saved to ./KMMheparams_target3/KMMheexperiments_transfer_tarnet_8_0.01_0.001.json\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_base_dir', type=str, help=\"path to directory LBIDD\", default=\"/Users/asus/Desktop/datasets\")\n",
    "    parser.add_argument('--knob', type=str, default='tarnet',\n",
    "                        help=\"dragonnet or tarnet\")\n",
    "\n",
    "    parser.add_argument('--output_base_dir', type=str, help=\"directory to save the output\",default=\"/Users/asus/Desktop/datasets\")\n",
    "\n",
    "    parser.add_argument('--transfer_lr',type = float,default=0.001)\n",
    "\n",
    "    parser.add_argument('--l1reg',type = float,default=0.01)\n",
    "\n",
    "    parser.add_argument('--batchsize',type = int,default=64)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    turn_knob(args.data_base_dir, args.knob, args.output_base_dir,args.transfer_lr, args.l1reg,args.batchsize)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48882c7-c654-4ff7-ac17-96e48fae4499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8709ade-8051-4d7e-b96f-b99ce718c60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
