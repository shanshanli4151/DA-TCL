{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ab115-21bf-4471-9438-9b052ffa3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics, svm, neighbors\n",
    "import scipy.linalg\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414f144-553e-46c2-8c18-86fbffaae33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(ker, X1, X2=None, gamma=1):\n",
    "    K = None\n",
    "    if not ker or ker == 'primal':\n",
    "        K = X1\n",
    "    elif ker == 'linear':\n",
    "        K = linear_kernel(np.asarray(X1).T, np.asarray(X2).T) if X2 is not None else linear_kernel(np.asarray(X1).T)\n",
    "    elif ker == 'rbf':\n",
    "        K = rbf_kernel(np.asarray(X1).T, np.asarray(X2).T, gamma) if X2 is not None else rbf_kernel(np.asarray(X1).T, None, gamma)\n",
    "    return K\n",
    "def proxy_a_distance(source_X, target_X):\n",
    "    nb_source = np.shape(source_X)[0]\n",
    "    nb_target = np.shape(target_X)[0]\n",
    "\n",
    "    train_X = np.vstack((source_X, target_X))\n",
    "    train_Y = np.hstack((np.zeros(nb_source, dtype=int), np.ones(nb_target, dtype=int)))\n",
    "\n",
    "    clf = svm.LinearSVC(random_state=0)\n",
    "    clf.fit(train_X, train_Y)\n",
    "    y_pred = clf.predict(train_X)\n",
    "    error = metrics.mean_absolute_error(train_Y, y_pred)\n",
    "    dist = 2 * (1 - 2 * error)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3475c-7f88-4054-8b1b-c14f7390fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mu(_X1, _Y1, _X2, _Y2):\n",
    "    adist_m = proxy_a_distance(_X1, _X2)\n",
    "    C = len(np.unique(_Y1))\n",
    "    epsilon = 1e-3\n",
    "    list_adist_c = []\n",
    "    for i in range(1, C + 1):\n",
    "        ind_i, ind_j = np.where(_Y1 == i), np.where(_Y2 == i)\n",
    "        Xsi = _X1[ind_i[0], :]\n",
    "        Xtj = _X2[ind_j[0], :]\n",
    "        adist_i = proxy_a_distance(Xsi, Xtj)\n",
    "        list_adist_c.append(adist_i)\n",
    "    adist_c = sum(list_adist_c) / C\n",
    "    mu = adist_c / (adist_c + adist_m)\n",
    "    mu = min(max(mu, epsilon), 1)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d577a6-766a-4dae-a67f-ef205a1e6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BDA_function(Xs, Ys, Xt, Yt, kernel_type='primal', dim=30, lamb=1, mu=0.5, gamma=1, T=10, mode='BDA', estimate_mu_flag=False):\n",
    "    X = np.hstack((Xs.T, Xt.T))\n",
    "    X /= np.linalg.norm(X, axis=0)\n",
    "    m, n = X.shape\n",
    "    ns, nt = len(Xs), len(Xt)\n",
    "    e = np.vstack((1 / ns * np.ones((ns, 1)), -1 / nt * np.ones((nt, 1))))\n",
    "    C = len(np.unique(Ys))\n",
    "    H = np.eye(n) - 1 / n * np.ones((n, n))\n",
    "    Y_tar_pseudo = None\n",
    "    Xs_new, Xt_new = None, None\n",
    "\n",
    "    for t in range(T):\n",
    "        M0 = e @ e.T * C\n",
    "        N = 0\n",
    "        if Y_tar_pseudo is not None and len(Y_tar_pseudo) == nt:\n",
    "            for c in range(1, C + 1):\n",
    "                if np.isnan(e).any() or np.isinf(e).any():\n",
    "                    raise ValueError(\"Vector 'e' contains NaNs or Infs.\")\n",
    "                e = np.zeros((n, 1))\n",
    "                Ns = len(Ys[np.where(Ys == c)])\n",
    "                Nt = len(Y_tar_pseudo[np.where(Y_tar_pseudo == c)])\n",
    "                if Ns == 0 or Nt == 0:\n",
    "\n",
    "                    continue\n",
    "                if mode == 'WBDA':\n",
    "                    Ps = Ns / len(Ys)\n",
    "                    Pt = Nt / len(Y_tar_pseudo)\n",
    "                    alpha = Pt / Ps\n",
    "                    mu = 1\n",
    "                else:\n",
    "                    alpha = 1\n",
    "\n",
    "                e[np.where(Ys == c)] = 1 / Ns\n",
    "                inds = np.where(Y_tar_pseudo == c)[0] + ns\n",
    "                e[inds] = -alpha / Nt\n",
    "                e[np.isinf(e)] = 0\n",
    "                N += np.dot(e, e.T)\n",
    "                \n",
    "\n",
    "        if estimate_mu_flag and mode == 'BDA':\n",
    "            if Xs_new is not None:\n",
    "                mu = estimate_mu(Xs_new, Ys, Xt_new, Y_tar_pseudo)\n",
    "            else:\n",
    "                mu = 0\n",
    "\n",
    "        M = (1 - mu) * M0 + mu * N\n",
    "    \n",
    "        norm_M = np.linalg.norm(M, 'fro')\n",
    "\n",
    "        if norm_M < 1e-10:\n",
    "            print(\"Warning: Frobenius norm of M is close to zero; skipping normalization.\")\n",
    "        else:\n",
    "            M /= norm_M  \n",
    "\n",
    "\n",
    "        if np.isnan(M).any() or np.isinf(M).any():\n",
    "            raise ValueError(\"Matrix 'M' contains NaNs or Infs after normalization.\")\n",
    "        #M /= np.linalg.norm(M, 'fro')\n",
    "        if np.isnan(M).any() or np.isinf(M).any():\n",
    "            raise ValueError(\"Matrix 'M' contains NaNs or Infs after construction.\")\n",
    "        K = kernel(kernel_type, X, None, gamma=gamma)\n",
    "        n_eye = m if kernel_type == 'primal' else n\n",
    "        if np.isnan(K).any() or np.isinf(K).any():\n",
    "            raise ValueError(\"Matrix 'K' contains NaNs or Infs.\")\n",
    "        if np.isnan(M).any() or np.isinf(M).any():\n",
    "            raise ValueError(\"Matrix 'M' contains NaNs or Infs.\")\n",
    "        if np.isnan(lamb).any() or np.isinf(lamb):\n",
    "            raise ValueError(\"Parameter 'lamb' contains NaNs or Infs.\")\n",
    "        a, b = K @ M @ K.T + lamb * np.eye(n_eye), K @ H @ K.T\n",
    "        if np.isnan(a).any() or np.isinf(a).any():\n",
    "            raise ValueError(\"Matrix 'a' contains NaNs or Infs.\")\n",
    "        if np.isnan(b).any() or np.isinf(b).any():\n",
    "            raise ValueError(\"Matrix 'b' contains NaNs or Infs.\")\n",
    "        w, V = scipy.linalg.eig(a, b)\n",
    "        ind = np.argsort(w)\n",
    "        A = V[:, ind[:dim]]\n",
    "        Z = A.T @ K\n",
    "        Z /= np.linalg.norm(Z, axis=0)\n",
    "        Xs_new, Xt_new = Z[:, :ns].T, Z[:, ns:].T\n",
    "\n",
    "        clf = neighbors.KNeighborsRegressor(n_neighbors=1)\n",
    "        clf.fit(Xs_new, Ys.ravel())\n",
    "        Y_tar_pseudo = clf.predict(Xt_new)\n",
    "\n",
    "    return Xs_new, Xt_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768494e9-6dcc-43e3-a789-8a9134a5af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ihdp_data import *\n",
    "from model import *\n",
    "import json\n",
    "import numpy as np\n",
    "from ate import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525789b9-bbc5-49f4-8cbb-d848f69f44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimate(q_t0, q_t1, g, t, y_dragon, index, eps, truncate_level=0.01):\n",
    "    \"\"\"\n",
    "    getting the back door adjustment & TMLE estimation\n",
    "    \"\"\"\n",
    "\n",
    "    psi_n = psi_naive(q_t0, q_t1, g, t, y_dragon, truncate_level=truncate_level)\n",
    "    ipw_n, dr_n = psi_weighting(q_t0, q_t1, g, t, y_dragon, truncate_level=truncate_level)\n",
    "    psi_tmle, psi_tmle_std, eps_hat, initial_loss, final_loss, g_loss = psi_tmle_cont_outcome(q_t0, q_t1, g, t,\n",
    "                                                                                              y_dragon,\n",
    "                                                                                              truncate_level=truncate_level)\n",
    "    return psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c2728-ad25-4a26-8db5-2d5a3a547e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_output(yt_hat, t, y, y_scaler, x, index):\n",
    "    \"\"\"     yt_hat: Generated prediction\n",
    "            t: Binary treatment assignments\n",
    "            y: Treatment outcomes\n",
    "            y_scaler: Scaled treatment outcomes\n",
    "            x: Covariates\n",
    "    \"\"\"\n",
    "    yt_hat = yt_hat.detach().cpu().numpy()\n",
    "    q_t0 = y_scaler.inverse_transform(yt_hat[:, 0].reshape(-1, 1).copy())\n",
    "    q_t1 = y_scaler.inverse_transform(yt_hat[:, 1].reshape(-1, 1).copy())\n",
    "    g = yt_hat[:, 2].copy()\n",
    "\n",
    "    if yt_hat.shape[1] == 4:\n",
    "        eps = yt_hat[:, 3][0]\n",
    "    else:\n",
    "        eps = np.zeros_like(yt_hat[:, 2])\n",
    "\n",
    "    y = y_scaler.inverse_transform(y.copy())\n",
    "    var = \"average propensity for treated: {} and untreated: {}\".format(g[t.squeeze() == 1.].mean(),\n",
    "                                                                        g[t.squeeze() == 0.].mean())\n",
    "    print(var)\n",
    "\n",
    "    return {'q_t0': q_t0, 'q_t1': q_t1, 'g': g, 't': t, 'y': y, 'x': x, 'index': index, 'eps': eps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723da35b-f38c-4122-9c5f-8bd3e81215a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, optimizer, criterion,valid_loader= None,l1_reg = None):\n",
    "\n",
    "    avg_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "  \n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if l1_reg is not None:\n",
    "            l1_penalty = l1_reg * sum([p.abs().sum() for p in net.parameters()])\n",
    "            loss+= l1_penalty\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "\n",
    "    valid_loss = None\n",
    "    if valid_loader is not None:\n",
    "        valid_loss = 0.0\n",
    "        net.eval()     \n",
    "        for data, labels in valid_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "            \n",
    "            target = net(data)\n",
    "            loss = criterion(target,labels)\n",
    "            if l1_reg is not None:\n",
    "                loss+= l1_reg * sum([p.abs().sum() for p in net.parameters()]) \n",
    "            valid_loss += loss\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "    return avg_loss / len(train_loader), valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c74cb-4293-4eca-b3b3-ef7962573a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_dragons(t, y_unscaled, x, net,seed = 0, targeted_regularization=True, output_dir='',\n",
    "                              knob_loss=dragonnet_loss_binarycross, ratio=1., dragon='', val_split=0.2, batch_size=64,lr =1e-3,l1_reg = None):\n",
    "    \"\"\"\n",
    "    Method for training dragonnet and tarnet and predicting new results\n",
    "    \"\"\"    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    verbose = 0\n",
    "    y_scaler = StandardScaler()\n",
    "    y = y_scaler.fit_transform(y_unscaled)\n",
    "    train_outputs = []\n",
    "    test_outputs = []\n",
    "    if targeted_regularization:\n",
    "        loss = make_tarreg_loss(ratio=ratio, dragonnet_loss=knob_loss)\n",
    "    else:\n",
    "        loss = knob_loss\n",
    "\n",
    "\n",
    "    i = seed\n",
    "    torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "    if ratio == 0:\n",
    "        train_index = np.arange(x.shape[0])\n",
    "        test_index = train_index\n",
    "    else:\n",
    "        train_index, test_index = train_test_split(np.arange(x.shape[0]), test_size=ratio, random_state=seed)\n",
    "        print(f'test_index {test_index}')\n",
    "   \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    t_train, t_test = t[train_index], t[test_index]\n",
    "\n",
    "    yt_train = np.concatenate([y_train, t_train], 1)\n",
    "\n",
    "    yt_test = np.concatenate([y_test, t_test], 1)\n",
    "\n",
    "    # Create data loader to pass onto training method\n",
    "    tensors_train = torch.from_numpy(x_train).float().to(device), torch.from_numpy(yt_train).float().to(device)\n",
    "    train_size = int((val_split) * len(TensorDataset(*tensors_train)))\n",
    "    val_size = int(len(TensorDataset(*tensors_train))-train_size)\n",
    "    train_set, valid_set = random_split(TensorDataset(*tensors_train),[train_size,val_size])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=500)\n",
    "\n",
    "    import time;\n",
    "    start_time = time.time()\n",
    "\n",
    "    epochs1 = 100\n",
    "    epochs2 = 300\n",
    "\n",
    "    optimizer_Adam = optim.Adam([{'params': net.representation_block.parameters()},\n",
    "                                 {'params': net.t_predictions.parameters()},\n",
    "                                 {'params': net.t0_head.parameters(), 'weight_decay': 0.01},\n",
    "                                 {'params': net.t1_head.parameters(), 'weight_decay': 0.01}], lr=lr)\n",
    "    optimizer_SGD = optim.SGD([{'params': net.representation_block.parameters()},\n",
    "                               {'params': net.t_predictions.parameters()},\n",
    "                               {'params': net.t0_head.parameters(), 'weight_decay': 0.01},\n",
    "                               {'params': net.t1_head.parameters(), 'weight_decay': 0.01}], lr=lr*0.01, momentum=0.9)\n",
    "    scheduler_Adam = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_Adam, mode='min', factor=0.5, patience=5,\n",
    "                                                          threshold=1e-8, cooldown=0, min_lr=0)\n",
    "    scheduler_SGD = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_SGD, mode='min', factor=0.5, patience=5,\n",
    "                                                         threshold=0, cooldown=0, min_lr=0)\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    early_stopper = EarlyStopper(patience=2, min_delta=0.)\n",
    "\n",
    "    # Adam training run\n",
    "    for epoch in range(epochs1):\n",
    "\n",
    "        # Train on data\n",
    "        train_loss,val_loss = train(train_loader, net, optimizer_Adam, loss,valid_loader = valid_loader,l1_reg = l1_reg)\n",
    "        \n",
    "        if early_stopper.early_stop(val_loss):             \n",
    "            break\n",
    "\n",
    "        scheduler_Adam.step(val_loss)\n",
    "\n",
    "    print(f\"Adam loss: train -- {train_loss}, validation -- {val_loss}, epoch {epoch}\")\n",
    "\n",
    "    # SGD training run\n",
    "    \n",
    "    early_stopper = EarlyStopper(patience=40, min_delta=0.)\n",
    "\n",
    "    for epoch in range(epochs2):\n",
    "        # Train on data\n",
    "        train_loss,val_loss = train(train_loader, net, optimizer_SGD, loss,valid_loader = valid_loader,l1_reg = l1_reg)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):             \n",
    "            break\n",
    "        scheduler_SGD.step(val_loss)\n",
    "        \n",
    "\n",
    "    print(f\"SGD loss: train --  {train_loss}, validation -- {val_loss},  epoch {epoch}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"***************************** elapsed_time is: \", elapsed_time)\n",
    "\n",
    "    yt_hat_test = net(torch.from_numpy(x_test).float().to(device))\n",
    "    yt_hat_train = net(torch.from_numpy(x_train).float().to(device))\n",
    "\n",
    "    test_outputs += [_split_output(yt_hat_test, t_test, y_test, y_scaler, x_test, test_index)]\n",
    "    train_outputs += [_split_output(yt_hat_train, t_train, y_train, y_scaler, x_train, train_index)]\n",
    "   \n",
    "    train_all_dicts = _split_output(yt_hat_train, t_train, y_train, y_scaler, x_train, train_index)\n",
    "    test_all_dicts = _split_output(yt_hat_test, t_test, y_test, y_scaler, x_test, test_index)\n",
    "   \n",
    "    psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n = get_estimate(train_all_dicts['q_t0'].reshape(-1, 1), train_all_dicts['q_t1'].reshape(-1, 1), train_all_dicts['g'].reshape(-1, 1), train_all_dicts['t'].reshape(-1, 1), train_all_dicts['y'].reshape(-1, 1), train_all_dicts['index'].reshape(-1, 1), train_all_dicts['eps'].reshape(-1, 1),truncate_level=0.01)\n",
    "\n",
    "    train_dict = {'psi_n':psi_n, 'classification_mse': g_loss,'ipw_n':ipw_n, 'dr_n':dr_n,'regression_loss':regression_loss(torch.tensor(yt_train).to(device),yt_hat_train).cpu().detach(),'BCE':binary_classification_loss(torch.tensor(yt_train).float().to(device),yt_hat_train).cpu().detach().numpy(),'regression_mse':initial_loss,'index':train_all_dicts['index']}\n",
    "    \n",
    "    psi_n, psi_tmle, initial_loss, final_loss, g_loss,ipw_n, dr_n = get_estimate(test_all_dicts['q_t0'].reshape(-1, 1), test_all_dicts['q_t1'].reshape(-1, 1), test_all_dicts['g'].reshape(-1, 1), test_all_dicts['t'].reshape(-1, 1), test_all_dicts['y'].reshape(-1, 1), test_all_dicts['index'].reshape(-1, 1), test_all_dicts['eps'].reshape(-1, 1),truncate_level=0.01)\n",
    "\n",
    "    \n",
    "    test_dict = {'psi_n':psi_n, 'classification_mse': g_loss,'ipw_n':ipw_n, 'dr_n':dr_n,'regression_loss':regression_loss(torch.tensor(yt_test).to(device),yt_hat_test).cpu().detach(),'BCE':binary_classification_loss(torch.tensor(yt_test).float().to(device),yt_hat_test).cpu().detach().numpy(),'regression_mses':initial_loss,'index':test_all_dicts['index']}\n",
    "\n",
    "    return test_outputs, train_outputs, net,train_dict,test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f534b-9f7d-40a5-9418-5edcf4ea7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.cuda.is_available = lambda: False\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Forcing CPU-only mode\")\n",
    "def run_BDAMerge(data_base_dir='/Users/asus/Desktop/datasets', output_dir='/Users/asus/Desktop/datasets',\n",
    "                knob_loss=dragonnet_loss_binarycross,\n",
    "                ratio=1, dragon='', lr2=1e-3, l1_reg=1e-3, batchsize2=64):\n",
    "\n",
    "    print(\"the dragon is {}\".format(dragon))\n",
    "    device = torch.device(\"cpu\")\n",
    "    simulation_files = sorted(glob.glob(\"{}/*.csv\".format(data_base_dir)))\n",
    "    \n",
    "    # 初始化列表用于收集所有测试误差\n",
    "    all_err_test = []\n",
    "    all_dr_err_test = []\n",
    "    all_ipw_error_test = []\n",
    "    \n",
    "    final_output = []\n",
    "    for idx, simulation_file in enumerate(simulation_files):\n",
    "        try:\n",
    "            print(f\"\\nProcessing file {idx+1}/{len(simulation_files)}: {os.path.basename(simulation_file)}\")\n",
    "            \n",
    "            # 加载特征和其他变量\n",
    "            x = load_and_format_covariates_ihdp(simulation_file)\n",
    "            t, y, y_cf, mu_0, mu_1 = load_all_other_crap(simulation_file)\n",
    "            \n",
    "            # 超参数设置\n",
    "            batchsize = 64\n",
    "            lr = 1e-3\n",
    "            test_ratio = 0.5\n",
    "            val_split = 0.3\n",
    "            batchsize2 = batchsize2\n",
    "            lr2 = lr2\n",
    "            l1_reg = l1_reg\n",
    "            \n",
    "            # 选择目标索引\n",
    "            target_col_idx = 3\n",
    "            target_idx0 = np.where(x[:, target_col_idx] == 0)[0]  # Source domain indices\n",
    "            target_idx1 = np.where(x[:, target_col_idx] == 1)[0]  # Target domain indices\n",
    "\n",
    "            # 划分源域和目标域数据\n",
    "            x_s = x[target_idx0]\n",
    "            y_s = y[target_idx0]\n",
    "            t_s = t[target_idx0]\n",
    "            y_cf_s = y_cf[target_idx0]\n",
    "            mu_0_s = mu_0[target_idx0]\n",
    "            mu_1_s = mu_1[target_idx0]\n",
    "\n",
    "            x_t = x[target_idx1]\n",
    "            y_t = y[target_idx1]\n",
    "            t_t = t[target_idx1]\n",
    "            y_cf_t = y_cf[target_idx1]\n",
    "            mu_0_t = mu_0[target_idx1]\n",
    "            mu_1_t = mu_1[target_idx1]\n",
    "\n",
    "            # 通过 BDA 进行域适配\n",
    "            Xs_new, Xt_new = BDA_function(x_s, y_s, x_t, y_t, kernel_type='primal', dim=30, lamb=1, mu=0.5, gamma=1, T=10, mode='BDA', estimate_mu_flag=True)\n",
    "            \n",
    "            # 训练源领域数据\n",
    "            for is_targeted_regularization in [False]:\n",
    "                print(\"Is targeted regularization: {}\".format(is_targeted_regularization))\n",
    "                torch.manual_seed(idx)\n",
    "\n",
    "                if dragon == 'tarnet':\n",
    "                    print('Creating TarNet model')\n",
    "                    net = TarNet(x.shape[1]).to(device)\n",
    "\n",
    "                elif dragon == 'dragonnet':\n",
    "                    print(\"Creating DragonNet model\")\n",
    "                    net = DragonNet(x.shape[1]).to(device)\n",
    "\n",
    "                # 使用更新后的 x_s（即 Xs_new）训练模型\n",
    "                _, _, net, _, _ = train_and_predict_dragons(t_s, y_s, Xs_new, net, seed=idx,\n",
    "                                                           targeted_regularization=is_targeted_regularization,\n",
    "                                                           knob_loss=knob_loss, ratio=0, dragon=dragon,\n",
    "                                                           val_split=val_split, batch_size=batchsize, lr=lr)\n",
    "                \n",
    "                # 保存基模型参数\n",
    "                parm = {}\n",
    "                for name, param in net.named_parameters():\n",
    "                    param.grad = None\n",
    "                    parm[name] = param.detach().cpu()  # 确保在CPU上\n",
    "\n",
    "                # 迁移学习阶段\n",
    "                if dragon == 'tarnet':\n",
    "                    print('Creating TarNet_transfer model')\n",
    "                    net = TarNet_transfer(x.shape[1], parm).to(device)\n",
    "\n",
    "                elif dragon == 'dragonnet':\n",
    "                    print(\"Creating DragonNet_transfer model\")\n",
    "                    net = DragonNet_transfer(x.shape[1], parm).to(device)\n",
    "\n",
    "                # 在目标域数据上进行二次训练（使用BDA转换后的目标域数据）\n",
    "                test_outputs, train_output, net, train_dict, test_dict = train_and_predict_dragons(\n",
    "                    t_t, y_t, Xt_new, net, seed=idx, targeted_regularization=is_targeted_regularization,\n",
    "                    knob_loss=knob_loss, ratio=test_ratio, dragon=dragon,\n",
    "                    val_split=val_split, batch_size=batchsize2, lr=lr2, l1_reg=l1_reg)\n",
    "\n",
    "                # 计算误差\n",
    "                for data_dict in [train_dict, test_dict]:\n",
    "                    # 确保索引在范围内\n",
    "                    max_index = len(mu_1_t) - 1\n",
    "                    valid_indices = [i for i in data_dict['index'] if 0 <= i <= max_index]\n",
    "                    \n",
    "                    if not valid_indices:\n",
    "                        print(f\"Warning: No valid indices in dict, skipping error calculation\")\n",
    "                        continue\n",
    "                        \n",
    "                    truth = (mu_1_t[valid_indices] - mu_0_t[valid_indices]).mean()\n",
    "                    \n",
    "                    # 确保预测值存在\n",
    "                    if 'psi_n' not in data_dict or 'dr_n' not in data_dict or 'ipw_n' not in data_dict:\n",
    "                        print(f\"Warning: Missing prediction values in dict, skipping error calculation\")\n",
    "                        continue\n",
    "                        \n",
    "                    data_dict['err'] = abs(truth - data_dict['psi_n']).mean()\n",
    "                    data_dict['dr_err'] = abs(truth - data_dict['dr_n']).mean()\n",
    "                    data_dict['ipw_error'] = abs(truth - data_dict['ipw_n']).mean()\n",
    "                    \n",
    "                    # 如果是测试集，收集误差用于最终统计\n",
    "                    if data_dict is test_dict:\n",
    "                        all_err_test.append(data_dict['err'])\n",
    "                        all_dr_err_test.append(data_dict['dr_err'])\n",
    "                        all_ipw_error_test.append(data_dict['ipw_error'])\n",
    "                \n",
    "                # 将索引转换为列表\n",
    "                test_dict['index'] = test_dict['index'].tolist()\n",
    "                train_dict['index'] = train_dict['index'].tolist()\n",
    "                \n",
    "                \n",
    "                train_dict_formatted = {f'{k}_train': v.item() if 'index' not in k else v for k, v in train_dict.items()}\n",
    "                test_dict_formatted = {f'{k}_test': v.item() if 'index' not in k else v for k, v in test_dict.items()}\n",
    "                \n",
    "                combined_dict = {**train_dict_formatted, **test_dict_formatted}\n",
    "                combined_dict['sim_idx'] = idx\n",
    "                final_output.append(combined_dict)\n",
    "                \n",
    "        \n",
    "                print(f\"Simulation {idx} results:\")\n",
    "                print(f\"  Test err: {combined_dict.get('err_test', 'N/A'):.4f}\")\n",
    "                print(f\"  Test dr_err: {combined_dict.get('dr_err_test', 'N/A'):.4f}\")\n",
    "                print(f\"  Test ipw_error: {combined_dict.get('ipw_error_test', 'N/A'):.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {simulation_file}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # 计算所有数据集的误差统计\n",
    "    if all_err_test:\n",
    "        err_mean = np.mean(all_err_test)\n",
    "        err_var = np.var(all_err_test)\n",
    "        dr_err_mean = np.mean(all_dr_err_test)\n",
    "        dr_err_var = np.var(all_dr_err_test)\n",
    "        ipw_err_mean = np.mean(all_ipw_error_test)\n",
    "        ipw_err_var = np.var(all_ipw_error_test)\n",
    "    else:\n",
    "        \n",
    "        err_mean = err_var = dr_err_mean = dr_err_var = ipw_err_mean = ipw_err_var = -1\n",
    "        print(\"WARNING: No valid test results were collected\")\n",
    "    \n",
    "    # 添加汇总统计到输出\n",
    "    summary = {\n",
    "        'err_mean': float(err_mean),\n",
    "        'err_variance': float(err_var),\n",
    "        'dr_err_mean': float(dr_err_mean),\n",
    "        'dr_err_variance': float(dr_err_var),\n",
    "        'ipw_err_mean': float(ipw_err_mean),\n",
    "        'ipw_err_variance': float(ipw_err_var),\n",
    "        'successful_runs': len(all_err_test)\n",
    "    }\n",
    "    final_output.append({'summary': summary})\n",
    "    \n",
    "    # 保存结果\n",
    "    if not os.path.exists(f'./BDAheparams_target{target_col_idx}/'):\n",
    "        os.makedirs(f'./BDA_merge_params_target{target_col_idx}/')\n",
    "    \n",
    "    output_file = f'./BDA_merge_params_target{target_col_idx}/BDAexperiments_transfer_{dragon}_{batchsize2}_{l1_reg}_{lr2}.json'\n",
    "    \n",
    "    \n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, torch.Tensor):\n",
    "                return obj.detach().cpu().numpy().tolist()\n",
    "            return super(NumpyEncoder, self).default(obj)\n",
    "    \n",
    "    with open(output_file, 'w') as fp:\n",
    "        json.dump(final_output, fp, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"BDA Error Summary for All Simulations:\")\n",
    "    print(f\"Successful runs: {len(all_err_test)}/{len(simulation_files)}\")\n",
    "    print(f\"ATE Error: Mean = {err_mean:.4f}, Variance = {err_var:.4f}\")\n",
    "    print(f\"DR Error: Mean = {dr_err_mean:.4f}, Variance = {dr_err_var:.4f}\")\n",
    "    print(f\"IPW Error: Mean = {ipw_err_mean:.4f}, Variance = {ipw_err_var:.4f}\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e401d43f-8911-4b41-8f42-e09b8492112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_knob(data_base_dir='/Users/asus/Desktop/datasets/', knob='dragonnet',\n",
    "              output_base_dir='',lr  = 1e-3, l1reg = 1e-4,batchsize = 64):\n",
    "    output_dir = os.path.join(output_base_dir, knob)\n",
    "\n",
    "    if knob == 'dragonnet':\n",
    "        run_BDAMerge(data_base_dir=data_base_dir, output_dir=output_dir, dragon='dragonnet' ,lr2  = lr ,l1_reg = l1reg, batchsize2 = batchsize)\n",
    "\n",
    "    if knob == 'tarnet':\n",
    "        run_BDAMerge(data_base_dir=data_base_dir, output_dir=output_dir, dragon='tarnet',lr2  = lr ,l1_reg = l1reg, batchsize2 = batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5939e2-2cd4-4b89-ae85-8637c6041686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_base_dir', type=str, help=\"path to directory LBIDD\", default=\"/Users/asus/Desktop/datasets\")\n",
    "    parser.add_argument('--knob', type=str, default='dragonnet',\n",
    "                        help=\"dragonnet or tarnet\")\n",
    "\n",
    "    parser.add_argument('--output_base_dir', type=str, help=\"directory to save the output\",default=\"/Users/asus/Desktop/datasets\")\n",
    "\n",
    "    parser.add_argument('--transfer_lr',type = float,default=0.001)\n",
    "\n",
    "    parser.add_argument('--l1reg',type = float,default=0.01)\n",
    "\n",
    "    parser.add_argument('--batchsize',type = int,default=64)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    turn_knob(args.data_base_dir, args.knob, args.output_base_dir,args.transfer_lr, args.l1reg,args.batchsize)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
